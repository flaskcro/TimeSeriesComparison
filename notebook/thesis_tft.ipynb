{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install darts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OVquhBylqd6k",
        "outputId": "644471ae-0ce2-41c3-e5c9-6fe1fa29fb22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting darts\n",
            "  Downloading darts-0.23.1-py3-none-any.whl (592 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m592.0/592.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from darts) (1.21.6)\n",
            "Collecting matplotlib>=3.3.0\n",
            "  Downloading matplotlib-3.6.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting statsforecast>=1.0.0\n",
            "  Downloading statsforecast-1.4.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: holidays>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from darts) (0.18)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from darts) (1.13.1+cu116)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from darts) (1.0.2)\n",
            "Collecting lightgbm>=3.2.0\n",
            "  Downloading lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nfoursid>=1.0.0\n",
            "  Downloading nfoursid-1.0.1-py3-none-any.whl (16 kB)\n",
            "Collecting pmdarima>=1.8.0\n",
            "  Downloading pmdarima-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.8/dist-packages (from darts) (4.64.1)\n",
            "Collecting statsmodels>=0.13.0\n",
            "  Downloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyod>=0.9.5\n",
            "  Downloading pyod-1.0.7.tar.gz (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.7/147.7 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting catboost>=1.0.6\n",
            "  Downloading catboost-1.1.1-cp38-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from darts) (1.3.5)\n",
            "Requirement already satisfied: xarray>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from darts) (2022.12.0)\n",
            "Collecting shap>=0.40.0\n",
            "  Downloading shap-0.41.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (575 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.9/575.9 KB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from darts) (1.7.3)\n",
            "Collecting pytorch-lightning>=1.5.0\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from darts) (1.2.0)\n",
            "Requirement already satisfied: prophet>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from darts) (1.1.1)\n",
            "Collecting xgboost>=1.6.0\n",
            "  Downloading xgboost-1.7.3-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.8/dist-packages (from darts) (2.25.1)\n",
            "Collecting tbats>=1.1.0\n",
            "  Downloading tbats-1.1.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from catboost>=1.0.6->darts) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (from catboost>=1.0.6->darts) (5.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (from catboost>=1.0.6->darts) (0.10.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from holidays>=0.11.1->darts) (2.8.2)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.8/dist-packages (from holidays>=0.11.1->darts) (0.3.1)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.8/dist-packages (from holidays>=0.11.1->darts) (2.2.4)\n",
            "Requirement already satisfied: convertdate>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from holidays>=0.11.1->darts) (2.4.0)\n",
            "Requirement already satisfied: PyMeeus in /usr/local/lib/python3.8/dist-packages (from holidays>=0.11.1->darts) (0.5.12)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from lightgbm>=3.2.0->darts) (0.38.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->darts) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->darts) (0.11.0)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->darts) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->darts) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->darts) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->darts) (2022.7)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.8/dist-packages (from pmdarima>=1.8.0->darts) (0.29.33)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from pmdarima>=1.8.0->darts) (1.24.3)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.8/dist-packages (from pmdarima>=1.8.0->darts) (57.4.0)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.8/dist-packages (from prophet>=1.1.1->darts) (0.0.9)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.8/dist-packages (from prophet>=1.1.1->darts) (1.0.8)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.8/dist-packages (from prophet>=1.1.1->darts) (1.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.8/dist-packages (from pyod>=0.9.5->darts) (0.56.4)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning>=1.5.0->darts) (2022.11.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning>=1.5.0->darts) (4.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning>=1.5.0->darts) (6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->darts) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->darts) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->darts) (4.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.1->darts) (3.1.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from shap>=0.40.0->darts) (2.2.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.13.0->darts) (0.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (3.8.3)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.8/dist-packages (from LunarCalendar>=0.0.9->prophet>=1.1.1->darts) (4.1.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (6.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.39.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly->catboost>=1.0.6->darts) (8.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (22.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.51->pyod>=0.9.5->darts) (3.11.0)\n",
            "Building wheels for collected packages: pyod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.0.7-py3-none-any.whl size=181101 sha256=b7d7f76646c32bab32ab9bdd61d0bd2e0daa13725b63aaaa19de9e93d7968782\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/e2/c1/1c7fd8b261e72411f6509afb429c84532e40ddcd96074473f4\n",
            "Successfully built pyod\n",
            "Installing collected packages: slicer, fonttools, contourpy, xgboost, torchmetrics, matplotlib, lightning-utilities, statsmodels, shap, nfoursid, lightgbm, catboost, statsforecast, pytorch-lightning, pyod, pmdarima, tbats, darts\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.12.2\n",
            "    Uninstalling statsmodels-0.12.2:\n",
            "      Successfully uninstalled statsmodels-0.12.2\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed catboost-1.1.1 contourpy-1.0.7 darts-0.23.1 fonttools-4.38.0 lightgbm-3.3.5 lightning-utilities-0.6.0.post0 matplotlib-3.6.3 nfoursid-1.0.1 pmdarima-2.0.2 pyod-1.0.7 pytorch-lightning-1.9.0 shap-0.41.0 slicer-0.0.7 statsforecast-1.4.0 statsmodels-0.13.5 tbats-1.1.2 torchmetrics-0.11.1 xgboost-1.7.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhJ9syeyqmJC",
        "outputId": "611c00f0-43e9-40ae-d8c2-38712bff5cfd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hg4IfRqoqccv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import darts\n",
        "from darts import TimeSeries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "from datetime import datetime\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts.metrics import smape, r2_score, rmse\n",
        "\n",
        "from darts.models import (\n",
        "    AutoARIMA,\n",
        "    Prophet,\n",
        "    RNNModel,\n",
        "    NBEATSModel,\n",
        "    BlockRNNModel,\n",
        "    TFTModel,\n",
        "    TransformerModel\n",
        ")\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from darts.utils.likelihood_models import QuantileRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CkKpfqmpqccx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/train.csv', parse_dates=['Date'])\n",
        "df.sample(10)\n",
        "df = pd.concat([df.drop(columns='StateHoliday'), pd.get_dummies(df.StateHoliday, prefix='Holiday')], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h0qzN_NZqccy"
      },
      "outputs": [],
      "source": [
        "def get_sample(num):\n",
        "    sample = df[df.Store == num]\n",
        "    series = TimeSeries.from_dataframe(sample, 'Date', 'Sales')\n",
        "\n",
        "    train, test = series.split_before(pd.Timestamp(\"20150601\"))\n",
        "\n",
        "    transformer = Scaler()\n",
        "    train_transformed = transformer.fit_transform(train)\n",
        "    test_transformed = transformer.transform(test)\n",
        "\n",
        "    series_customers = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Customers')\n",
        "    series_open = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Open')\n",
        "    series_promo = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Promo')\n",
        "    series_school = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='SchoolHoliday')\n",
        "    series_weekday = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='DayOfWeek')\n",
        "    series_holiday_a = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Holiday_a')\n",
        "    series_holiday_b = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Holiday_b')\n",
        "    series_holiday_c = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Holiday_c')\n",
        "\n",
        "    customers_transformed = transformer.transform(series_customers)\n",
        "\n",
        "    covariates = series_customers.stack(series_open)\n",
        "    covariates = covariates.stack(series_promo)\n",
        "    covariates = covariates.stack(series_school)\n",
        "    covariates = covariates.stack(series_weekday)\n",
        "    covariates = covariates.stack(series_holiday_a)\n",
        "    covariates = covariates.stack(series_holiday_b)\n",
        "    covariates = covariates.stack(series_holiday_c)\n",
        "\n",
        "    train_covariates, test_covariates = covariates.split_before(pd.Timestamp(\"20150601\"))\n",
        "\n",
        "    return train_transformed, test_transformed, covariates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xOpzQJRwqccy"
      },
      "outputs": [],
      "source": [
        "train, test, covariates = get_sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QpQHwAPSqccz"
      },
      "outputs": [],
      "source": [
        "def rmspe(actual, pred):\n",
        "    return np.sqrt(np.mean( ((actual - pred) / actual)**2)) \n",
        "\n",
        "def smape(actual, pred):\n",
        "    return np.mean(np.abs(pred - actual) / ((np.abs(actual) + np.abs(pred))/2)) * 100\n",
        "\n",
        "def evaluate_model(model, train, test):\n",
        "    model.fit(train)\n",
        "    pred = model.predict(len(test))\n",
        "    test = test.pd_dataframe()\n",
        "    pred = pred.pd_dataframe()\n",
        "\n",
        "    test.columns = ['Actual']\n",
        "    pred.columns = ['Pred']\n",
        "\n",
        "    df = pd.concat([test, pred], axis=1)\n",
        "    df = df[df.Actual > 0]\n",
        "    return smape(df.Actual.values, df.Pred.values), rmspe(df.Actual.values, df.Pred.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z46OgzySqccz"
      },
      "outputs": [],
      "source": [
        "store_num_list = pd.read_csv('/content/drive/MyDrive/store_list.csv').Store.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGHj0YZyqccz"
      },
      "source": [
        "## 1. Transfer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NFseRaSZqcc0"
      },
      "outputs": [],
      "source": [
        "def evaluate_covariates_model(model, train, test, covariates=None):\n",
        "    if covariates is not None:\n",
        "        model.fit(\n",
        "            train,\n",
        "            future_covariates=covariates,\n",
        "            verbose=False,\n",
        "        )\n",
        "    else:\n",
        "        model.fit(\n",
        "            train,\n",
        "            verbose=False,\n",
        "        )\n",
        "    if covariates is not None:\n",
        "        pred = model.predict(len(test), future_covariates=covariates)\n",
        "    else:\n",
        "        pred = model.predict(len(test))\n",
        "    pred = pred.pd_dataframe()\n",
        "    test = test.pd_dataframe()\n",
        "\n",
        "    test.columns = ['Actual']\n",
        "    pred.columns = ['Pred']\n",
        "\n",
        "    df = pd.concat([test, pred], axis=1)\n",
        "    df = df[df.Actual > 0]\n",
        "    return smape(df.Actual.values, df.Pred.values), rmspe(df.Actual.values, df.Pred.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9fsen9Afqcc0"
      },
      "outputs": [],
      "source": [
        "quantiles = [\n",
        "    0.01,\n",
        "    0.05,\n",
        "    0.1,\n",
        "    0.15,\n",
        "    0.2,\n",
        "    0.25,\n",
        "    0.3,\n",
        "    0.4,\n",
        "    0.5,\n",
        "    0.6,\n",
        "    0.7,\n",
        "    0.75,\n",
        "    0.8,\n",
        "    0.85,\n",
        "    0.9,\n",
        "    0.95,\n",
        "    0.99,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFEcKbWwqcc0",
        "outputId": "6383af85-9012-4c39-8bbc-5bea64ce1099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 8.094236439846897 0.1037844591387345\n"
          ]
        }
      ],
      "source": [
        "smape_list = []\n",
        "rmspe_list = []\n",
        "\n",
        "for idx, num in enumerate(store_num_list):\n",
        "    if idx % 2 == 1:\n",
        "        continue\n",
        "    train_transformed, test_transformed, covariates = get_sample(num)\n",
        "    tft_model = TFTModel(\n",
        "        input_chunk_length=28,\n",
        "        output_chunk_length=7,\n",
        "        hidden_size=64,\n",
        "        lstm_layers=1,\n",
        "        num_attention_heads=4,\n",
        "        dropout=0.1,\n",
        "        batch_size=16,\n",
        "        n_epochs=300,\n",
        "        add_relative_index=False,\n",
        "        add_encoders=None,\n",
        "        likelihood=QuantileRegression(\n",
        "                quantiles=quantiles\n",
        "        ),  \n",
        "        random_state=42, \n",
        "        pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0], \"enable_progress_bar\": False}\n",
        "    )\n",
        "    \n",
        "    smape_score, rmspe_score = evaluate_covariates_model(tft_model, train_transformed, test_transformed, covariates )\n",
        "    rmspe_list.append(rmspe_score)\n",
        "    smape_list.append(smape_score)\n",
        "    print(num, smape_score, rmspe_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smape1_list = []\n",
        "rmspe1_list = []\n",
        "\n",
        "for num in tqdm(store_num_list):\n",
        "    train_transformed, test_transformed, covariates = get_sample(num)\n",
        "    tft_model = TFTModel(\n",
        "        input_chunk_length=28,\n",
        "        output_chunk_length=7,\n",
        "        hidden_size=64,\n",
        "        lstm_layers=1,\n",
        "        num_attention_heads=4,\n",
        "        dropout=0.1,\n",
        "        batch_size=16,\n",
        "        n_epochs=300,\n",
        "        add_relative_index=False,\n",
        "        add_encoders=None,\n",
        "        likelihood=QuantileRegression(\n",
        "                quantiles=quantiles\n",
        "        ),  \n",
        "        random_state=42, \n",
        "        pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0], \"enable_progress_bar\": False}\n",
        "    )\n",
        "    \n",
        "    smape_score, rmspe_score = evaluate_covariates_model(tft_model, train_transformed, test_transformed, covariates )\n",
        "    rmspe1_list.append(rmspe_score)\n",
        "    smape1_list.append(smape_score)\n",
        "    print(num, smape_score, rmspe_score)"
      ],
      "metadata": {
        "id": "SiDD7kw5ySKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kz8rQlTqcc1"
      },
      "outputs": [],
      "source": [
        "print(np.mean(smape_list), np.std(smape_list), np.max(smape_list), np.min(smape_list))\n",
        "plt.hist(smape_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWvgvKSXqcc1"
      },
      "outputs": [],
      "source": [
        "print(np.mean(rmspe_list), np.std(rmspe_list), np.max(rmspe_list), np.min(rmspe_list))\n",
        "plt.hist(rmspe_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynr7dgNqqcc1"
      },
      "outputs": [],
      "source": [
        "res = pd.DataFrame(\n",
        "    {\n",
        "        'without_rmspe' : rmspe_list,\n",
        "        'without_smape' : smape_list,\n",
        "        #'with_rmspe' : rmspe_list,\n",
        "        #'with_smape' : smape_list,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfyvprSaqcc1"
      },
      "source": [
        "### 1.3 Result Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtFIeEJnqcc1"
      },
      "outputs": [],
      "source": [
        "result = res.copy()\n",
        "result['Store'] = store_num_list\n",
        "result.to_csv('/content/drive/MyDrive/transformer_nocov_result.csv')\n",
        "result.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8gCFodpqcc1"
      },
      "outputs": [],
      "source": [
        "train_transformed, test_transformed, covariates = get_sample(1)\n",
        "tft_model = TFTModel(\n",
        "    input_chunk_length=28,\n",
        "    output_chunk_length=7,\n",
        "    hidden_size=64,\n",
        "    lstm_layers=1,\n",
        "    num_attention_heads=4,\n",
        "    dropout=0.1,\n",
        "    batch_size=16,\n",
        "    n_epochs=300,\n",
        "    add_relative_index=False,\n",
        "    add_encoders=None,\n",
        "    likelihood=QuantileRegression(\n",
        "        quantiles=quantiles\n",
        "    ),  \n",
        "    random_state=42, \n",
        "    pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0], \"enable_progress_bar\": False}\n",
        ")\n",
        "\n",
        "tft_model.fit(train_transformed)\n",
        "pred_demand_covs = tft_model.predict(len(test_transformed))\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "test_transformed.plot(label=\"actual\")\n",
        "pred_demand_covs.plot(label=\"forecast\")\n",
        "plt.title('Generic N-Beats')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26gICb6kqcc2"
      },
      "outputs": [],
      "source": [
        "train_transformed, test_transformed, covariates = get_sample(10)\n",
        "tft_model = TFTModel(\n",
        "    input_chunk_length=28,\n",
        "    output_chunk_length=7,\n",
        "    hidden_size=64,\n",
        "    lstm_layers=1,\n",
        "    num_attention_heads=4,\n",
        "    dropout=0.1,\n",
        "    batch_size=16,\n",
        "    n_epochs=300,\n",
        "    add_relative_index=False,\n",
        "    add_encoders=None,\n",
        "    likelihood=QuantileRegression(\n",
        "        quantiles=quantiles\n",
        "    ),  \n",
        "    random_state=42, \n",
        "    pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0], \"enable_progress_bar\": False}\n",
        ")\n",
        "\n",
        "tft_model.fit(train_transformed, future_covariates=covariates)\n",
        "pred_demand_covs = tft_model.predict(len(test_transformed), future_covariates=covariates)\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "test_transformed.plot(label=\"actual\")\n",
        "pred_demand_covs.plot(label=\"forecast\")\n",
        "plt.title('TFT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AjBXb5rqcc2"
      },
      "source": [
        "## Hyper Parameter Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mdYJUyX8qcc2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from torchmetrics import MeanAbsoluteError, MeanAbsolutePercentageError, MetricCollection, SymmetricMeanAbsolutePercentageError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mpThtjXAqcc2"
      },
      "outputs": [],
      "source": [
        "from darts.utils.likelihood_models import GaussianLikelihood\n",
        "\n",
        "def train_model(model_args, callbacks, train, val, covariates):\n",
        "    torch_metrics = MetricCollection([SymmetricMeanAbsolutePercentageError(), MeanAbsoluteError()])\n",
        "    # Create the model using model_args from Ray Tune\n",
        "    model = TFTModel(\n",
        "        input_chunk_length=7,\n",
        "        output_chunk_length=7,\n",
        "        lstm_layers=1,\n",
        "        num_attention_heads=4,\n",
        "        n_epochs=300,\n",
        "        add_relative_index=False,\n",
        "        add_encoders=None,\n",
        "        likelihood=QuantileRegression(\n",
        "                quantiles=quantiles\n",
        "        ),  \n",
        "        random_state=42, \n",
        "        torch_metrics=torch_metrics,\n",
        "        pl_trainer_kwargs ={\"callbacks\": callbacks, \"enable_progress_bar\": False},\n",
        "        **model_args\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        series=train,\n",
        "        val_series=val,\n",
        "        future_covariates=covariates,\n",
        "        val_future_covariates=covariates\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DKobHPqJqcc2"
      },
      "outputs": [],
      "source": [
        "train, test, covariates = get_sample(10)\n",
        "train, val = train.split_after(pd.Timestamp(year=2015, month=4, day=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xg5bAOZjqcc2",
        "outputId": "c8795ade-aaef-40be-b54d-71d52b20c55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-30 19:27:44,947\tINFO worker.py:1538 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:27:46 (running for 00:00:00.24)\n",
            "Memory usage on this node: 3.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 16/20 (15 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+\n",
            "| Trial name              | status   | loc               |   batch_size |   hidden_size |   dropout |\n",
            "|-------------------------+----------+-------------------+--------------+---------------+-----------|\n",
            "| train_model_2cb17_00000 | RUNNING  | 172.28.0.12:55343 |           32 |            64 |  0.127941 |\n",
            "| train_model_2cb17_00001 | PENDING  |                   |           32 |            48 |  0.241223 |\n",
            "| train_model_2cb17_00002 | PENDING  |                   |           32 |            48 |  0.225601 |\n",
            "| train_model_2cb17_00003 | PENDING  |                   |           64 |            32 |  0.112693 |\n",
            "| train_model_2cb17_00004 | PENDING  |                   |          128 |            32 |  0.149138 |\n",
            "| train_model_2cb17_00005 | PENDING  |                   |           32 |            48 |  0.291225 |\n",
            "| train_model_2cb17_00006 | PENDING  |                   |           64 |            48 |  0.270748 |\n",
            "| train_model_2cb17_00007 | PENDING  |                   |          128 |            64 |  0.275324 |\n",
            "| train_model_2cb17_00008 | PENDING  |                   |           32 |            32 |  0.109133 |\n",
            "| train_model_2cb17_00009 | PENDING  |                   |           16 |            32 |  0.18789  |\n",
            "| train_model_2cb17_00010 | PENDING  |                   |           32 |            48 |  0.130921 |\n",
            "| train_model_2cb17_00011 | PENDING  |                   |          128 |            64 |  0.24035  |\n",
            "| train_model_2cb17_00012 | PENDING  |                   |           32 |            48 |  0.278062 |\n",
            "| train_model_2cb17_00013 | PENDING  |                   |           64 |            48 |  0.166128 |\n",
            "| train_model_2cb17_00014 | PENDING  |                   |           16 |            64 |  0.104861 |\n",
            "| train_model_2cb17_00015 | PENDING  |                   |           32 |            32 |  0.299592 |\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 16.2 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 14.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 18 | output_layer                      | Linear                           | 1.1 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 238 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 238 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 1.908     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m   rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th style=\"text-align: right;\">   SMAPE</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname    </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_model_2cb17_00000</td><td style=\"text-align: right;\">0.522666</td><td>2023-01-30_19-28-16</td><td>True  </td><td>                </td><td>7360d7610368490992a6e91342cc48a3</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                        10</td><td>172.28.0.12</td><td style=\"text-align: right;\">55343</td><td style=\"text-align: right;\">            24.6291 </td><td style=\"text-align: right;\">          2.19586 </td><td style=\"text-align: right;\">      24.6291 </td><td style=\"text-align: right;\"> 1675106896</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>2cb17_00000</td><td style=\"text-align: right;\">   0.00367045</td></tr>\n",
              "<tr><td>train_model_2cb17_00001</td><td style=\"text-align: right;\">0.551715</td><td>2023-01-30_19-28-44</td><td>True  </td><td>                </td><td>d2ca005f2b7b4918ab637e3da9dd51c5</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                        10</td><td>172.28.0.12</td><td style=\"text-align: right;\">55576</td><td style=\"text-align: right;\">            22.1769 </td><td style=\"text-align: right;\">          1.95423 </td><td style=\"text-align: right;\">      22.1769 </td><td style=\"text-align: right;\"> 1675106924</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>2cb17_00001</td><td style=\"text-align: right;\">   0.00333333</td></tr>\n",
              "<tr><td>train_model_2cb17_00002</td><td style=\"text-align: right;\">0.532596</td><td>2023-01-30_19-29-16</td><td>True  </td><td>                </td><td>69d18292901546caa6dc6592dac8858c</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                        12</td><td>172.28.0.12</td><td style=\"text-align: right;\">55792</td><td style=\"text-align: right;\">            26.0517 </td><td style=\"text-align: right;\">          1.93021 </td><td style=\"text-align: right;\">      26.0517 </td><td style=\"text-align: right;\"> 1675106956</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>2cb17_00002</td><td style=\"text-align: right;\">   0.00321722</td></tr>\n",
              "<tr><td>train_model_2cb17_00003</td><td style=\"text-align: right;\">0.689665</td><td>2023-01-30_19-29-28</td><td>True  </td><td>                </td><td>f23dd23ea901410993f23c902111fa73</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">56028</td><td style=\"text-align: right;\">             6.28707</td><td style=\"text-align: right;\">          1.26671 </td><td style=\"text-align: right;\">       6.28707</td><td style=\"text-align: right;\"> 1675106968</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00003</td><td style=\"text-align: right;\">   0.00336671</td></tr>\n",
              "<tr><td>train_model_2cb17_00004</td><td style=\"text-align: right;\">0.783591</td><td>2023-01-30_19-29-39</td><td>True  </td><td>                </td><td>9d99d25a763b4f9a8d499af34d6a9ed2</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">56165</td><td style=\"text-align: right;\">             5.52578</td><td style=\"text-align: right;\">          0.999355</td><td style=\"text-align: right;\">       5.52578</td><td style=\"text-align: right;\"> 1675106979</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00004</td><td style=\"text-align: right;\">   0.00322628</td></tr>\n",
              "<tr><td>train_model_2cb17_00005</td><td style=\"text-align: right;\">0.539846</td><td>2023-01-30_19-30-09</td><td>True  </td><td>                </td><td>80134474dd78486eb0ffee0cb62ac5e1</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                        11</td><td>172.28.0.12</td><td style=\"text-align: right;\">56295</td><td style=\"text-align: right;\">            23.9467 </td><td style=\"text-align: right;\">          1.94528 </td><td style=\"text-align: right;\">      23.9467 </td><td style=\"text-align: right;\"> 1675107009</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  11</td><td>2cb17_00005</td><td style=\"text-align: right;\">   0.00309873</td></tr>\n",
              "<tr><td>train_model_2cb17_00006</td><td style=\"text-align: right;\">0.688991</td><td>2023-01-30_19-30-22</td><td>True  </td><td>                </td><td>eb750046d6ee40fa99429dca1bdc4b07</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">56520</td><td style=\"text-align: right;\">             6.89544</td><td style=\"text-align: right;\">          1.45701 </td><td style=\"text-align: right;\">       6.89544</td><td style=\"text-align: right;\"> 1675107022</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00006</td><td style=\"text-align: right;\">   0.00323153</td></tr>\n",
              "<tr><td>train_model_2cb17_00007</td><td style=\"text-align: right;\">0.789988</td><td>2023-01-30_19-30-34</td><td>True  </td><td>                </td><td>62bb72fb4d844d078c861290da773081</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">56663</td><td style=\"text-align: right;\">             6.6142 </td><td style=\"text-align: right;\">          1.35689 </td><td style=\"text-align: right;\">       6.6142 </td><td style=\"text-align: right;\"> 1675107034</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00007</td><td style=\"text-align: right;\">   0.00349879</td></tr>\n",
              "<tr><td>train_model_2cb17_00008</td><td style=\"text-align: right;\">0.556069</td><td>2023-01-30_19-30-53</td><td>True  </td><td>                </td><td>0b7147f195024bee969fe125481e6934</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         6</td><td>172.28.0.12</td><td style=\"text-align: right;\">56801</td><td style=\"text-align: right;\">            13.1521 </td><td style=\"text-align: right;\">          1.74976 </td><td style=\"text-align: right;\">      13.1521 </td><td style=\"text-align: right;\"> 1675107053</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>2cb17_00008</td><td style=\"text-align: right;\">   0.00314236</td></tr>\n",
              "<tr><td>train_model_2cb17_00009</td><td style=\"text-align: right;\">0.526454</td><td>2023-01-30_19-31-26</td><td>True  </td><td>                </td><td>93ca7f7c713941748b9a3396fc25fc3d</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         9</td><td>172.28.0.12</td><td style=\"text-align: right;\">56972</td><td style=\"text-align: right;\">            27.4255 </td><td style=\"text-align: right;\">          2.73378 </td><td style=\"text-align: right;\">      27.4255 </td><td style=\"text-align: right;\"> 1675107086</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>2cb17_00009</td><td style=\"text-align: right;\">   0.00331306</td></tr>\n",
              "<tr><td>train_model_2cb17_00010</td><td style=\"text-align: right;\">0.534799</td><td>2023-01-30_19-31-56</td><td>True  </td><td>                </td><td>d23d8c04e9af4a1687aefd991b53856a</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                        11</td><td>172.28.0.12</td><td style=\"text-align: right;\">57209</td><td style=\"text-align: right;\">            23.9819 </td><td style=\"text-align: right;\">          1.96551 </td><td style=\"text-align: right;\">      23.9819 </td><td style=\"text-align: right;\"> 1675107116</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  11</td><td>2cb17_00010</td><td style=\"text-align: right;\">   0.00312066</td></tr>\n",
              "<tr><td>train_model_2cb17_00011</td><td style=\"text-align: right;\">0.754726</td><td>2023-01-30_19-32-07</td><td>True  </td><td>                </td><td>b2d2d0500d814f23a7a5018d33f4fbee</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">57425</td><td style=\"text-align: right;\">             6.65919</td><td style=\"text-align: right;\">          1.36481 </td><td style=\"text-align: right;\">       6.65919</td><td style=\"text-align: right;\"> 1675107127</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00011</td><td style=\"text-align: right;\">   0.00335598</td></tr>\n",
              "<tr><td>train_model_2cb17_00012</td><td style=\"text-align: right;\">0.545067</td><td>2023-01-30_19-32-27</td><td>True  </td><td>                </td><td>03de953033d74a5fa7c775901e7495f2</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         6</td><td>172.28.0.12</td><td style=\"text-align: right;\">57563</td><td style=\"text-align: right;\">            14.4394 </td><td style=\"text-align: right;\">          1.97146 </td><td style=\"text-align: right;\">      14.4394 </td><td style=\"text-align: right;\"> 1675107147</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>2cb17_00012</td><td style=\"text-align: right;\">   0.00339532</td></tr>\n",
              "<tr><td>train_model_2cb17_00013</td><td style=\"text-align: right;\">0.669737</td><td>2023-01-30_19-32-40</td><td>True  </td><td>                </td><td>0bb35af108474dc287e1cd1f13ba4a14</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">57741</td><td style=\"text-align: right;\">             6.93582</td><td style=\"text-align: right;\">          1.43855 </td><td style=\"text-align: right;\">       6.93582</td><td style=\"text-align: right;\"> 1675107160</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00013</td><td style=\"text-align: right;\">   0.00313401</td></tr>\n",
              "<tr><td>train_model_2cb17_00014</td><td style=\"text-align: right;\">0.504148</td><td>2023-01-30_19-33-13</td><td>True  </td><td>                </td><td>b01bcfb1c3db4ddcbade5690f30c9304</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         8</td><td>172.28.0.12</td><td style=\"text-align: right;\">57878</td><td style=\"text-align: right;\">            28.2623 </td><td style=\"text-align: right;\">          3.20901 </td><td style=\"text-align: right;\">      28.2623 </td><td style=\"text-align: right;\"> 1675107193</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>2cb17_00014</td><td style=\"text-align: right;\">   0.00332427</td></tr>\n",
              "<tr><td>train_model_2cb17_00015</td><td style=\"text-align: right;\">0.654586</td><td>2023-01-30_19-33-27</td><td>True  </td><td>                </td><td>7dc9ef9268524a43aab5ba4228291ab5</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">58118</td><td style=\"text-align: right;\">             7.80244</td><td style=\"text-align: right;\">          1.75758 </td><td style=\"text-align: right;\">       7.80244</td><td style=\"text-align: right;\"> 1675107207</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00015</td><td style=\"text-align: right;\">   0.00320268</td></tr>\n",
              "<tr><td>train_model_2cb17_00016</td><td style=\"text-align: right;\">0.541078</td><td>2023-01-30_19-33-58</td><td>True  </td><td>                </td><td>99e9e645198d4edc8cb7072c21cde495</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         8</td><td>172.28.0.12</td><td style=\"text-align: right;\">58258</td><td style=\"text-align: right;\">            26.2223 </td><td style=\"text-align: right;\">          2.96985 </td><td style=\"text-align: right;\">      26.2223 </td><td style=\"text-align: right;\"> 1675107238</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>2cb17_00016</td><td style=\"text-align: right;\">   0.00353241</td></tr>\n",
              "<tr><td>train_model_2cb17_00017</td><td style=\"text-align: right;\">0.654403</td><td>2023-01-30_19-34-12</td><td>True  </td><td>                </td><td>f29e06186b634b11ba74e0c71ac42ada</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">58485</td><td style=\"text-align: right;\">             7.88578</td><td style=\"text-align: right;\">          1.80766 </td><td style=\"text-align: right;\">       7.88578</td><td style=\"text-align: right;\"> 1675107252</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00017</td><td style=\"text-align: right;\">   0.00317478</td></tr>\n",
              "<tr><td>train_model_2cb17_00018</td><td style=\"text-align: right;\">0.687745</td><td>2023-01-30_19-34-24</td><td>True  </td><td>                </td><td>28c34a898bb344b3b94308bbe0ca045e</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">58634</td><td style=\"text-align: right;\">             6.72991</td><td style=\"text-align: right;\">          1.38106 </td><td style=\"text-align: right;\">       6.72991</td><td style=\"text-align: right;\"> 1675107264</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00018</td><td style=\"text-align: right;\">   0.00322199</td></tr>\n",
              "<tr><td>train_model_2cb17_00019</td><td style=\"text-align: right;\">0.837275</td><td>2023-01-30_19-34-35</td><td>True  </td><td>                </td><td>4669a9b857524eb28029c4bdecc90bdd</td><td>ee5d02e41edb</td><td style=\"text-align: right;\">                         3</td><td>172.28.0.12</td><td style=\"text-align: right;\">58771</td><td style=\"text-align: right;\">             5.55963</td><td style=\"text-align: right;\">          1.01554 </td><td style=\"text-align: right;\">       5.55963</td><td style=\"text-align: right;\"> 1675107275</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2cb17_00019</td><td style=\"text-align: right;\">   0.00334215</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:27:56 (running for 00:00:10.05)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.7104479326208002 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 17/20 (16 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status   | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00000 | RUNNING  | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.710448 |                    1 |\n",
            "| train_model_2cb17_00001 | PENDING  |                   |           32 |            48 |  0.241223 |          |                      |\n",
            "| train_model_2cb17_00002 | PENDING  |                   |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING  |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING  |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING  |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING  |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING  |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING  |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING  |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING  |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING  |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING  |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING  |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING  |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING  |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING  |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:27:56,480\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:27:58,712\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:00,902\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:03 (running for 00:00:16.70)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5831603411439923 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 17/20 (16 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+---------+----------------------+\n",
            "| Trial name              | status   | loc               |   batch_size |   hidden_size |   dropout |   SMAPE |   training_iteration |\n",
            "|-------------------------+----------+-------------------+--------------+---------------+-----------+---------+----------------------|\n",
            "| train_model_2cb17_00000 | RUNNING  | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.58316 |                    4 |\n",
            "| train_model_2cb17_00001 | PENDING  |                   |           32 |            48 |  0.241223 |         |                      |\n",
            "| train_model_2cb17_00002 | PENDING  |                   |           32 |            48 |  0.225601 |         |                      |\n",
            "| train_model_2cb17_00003 | PENDING  |                   |           64 |            32 |  0.112693 |         |                      |\n",
            "| train_model_2cb17_00004 | PENDING  |                   |          128 |            32 |  0.149138 |         |                      |\n",
            "| train_model_2cb17_00005 | PENDING  |                   |           32 |            48 |  0.291225 |         |                      |\n",
            "| train_model_2cb17_00006 | PENDING  |                   |           64 |            48 |  0.270748 |         |                      |\n",
            "| train_model_2cb17_00007 | PENDING  |                   |          128 |            64 |  0.275324 |         |                      |\n",
            "| train_model_2cb17_00008 | PENDING  |                   |           32 |            32 |  0.109133 |         |                      |\n",
            "| train_model_2cb17_00009 | PENDING  |                   |           16 |            32 |  0.18789  |         |                      |\n",
            "| train_model_2cb17_00010 | PENDING  |                   |           32 |            48 |  0.130921 |         |                      |\n",
            "| train_model_2cb17_00011 | PENDING  |                   |          128 |            64 |  0.24035  |         |                      |\n",
            "| train_model_2cb17_00012 | PENDING  |                   |           32 |            48 |  0.278062 |         |                      |\n",
            "| train_model_2cb17_00013 | PENDING  |                   |           64 |            48 |  0.166128 |         |                      |\n",
            "| train_model_2cb17_00014 | PENDING  |                   |           16 |            64 |  0.104861 |         |                      |\n",
            "| train_model_2cb17_00015 | PENDING  |                   |           32 |            32 |  0.299592 |         |                      |\n",
            "| train_model_2cb17_00016 | PENDING  |                   |           16 |            48 |  0.155627 |         |                      |\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+---------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:03,134\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:05,327\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:07,516\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:09 (running for 00:00:23.28)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5626689682711528 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5550019262119062 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 17/20 (16 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status   | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00000 | RUNNING  | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.555002 |                    7 |\n",
            "| train_model_2cb17_00001 | PENDING  |                   |           32 |            48 |  0.241223 |          |                      |\n",
            "| train_model_2cb17_00002 | PENDING  |                   |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING  |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING  |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING  |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING  |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING  |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING  |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING  |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING  |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING  |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING  |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING  |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING  |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING  |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING  |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:09,709\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:11,933\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:14,153\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:16 (running for 00:00:29.91)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5626689682711528 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 17/20 (16 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status   | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00000 | RUNNING  | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | PENDING  |                   |           32 |            48 |  0.241223 |          |                      |\n",
            "| train_model_2cb17_00002 | PENDING  |                   |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING  |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING  |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING  |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING  |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING  |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING  |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING  |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING  |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING  |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING  |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING  |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING  |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING  |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING  |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "+-------------------------+----------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55343)\u001b[0m 2023-01-30 19:28:16,349\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:22 (running for 00:00:35.61)\n",
            "Memory usage on this node: 3.8/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5626689682711528 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 18/20 (16 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00001 | RUNNING    | 172.28.0.12:55576 |           32 |            48 |  0.241223 |          |                      |\n",
            "| train_model_2cb17_00002 | PENDING    |                   |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:26,510\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:28 (running for 00:00:42.06)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5626689682711528 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 18/20 (16 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00001 | RUNNING    | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.691115 |                    2 |\n",
            "| train_model_2cb17_00002 | PENDING    |                   |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:28,495\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:30,455\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:32,417\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:34 (running for 00:00:47.95)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5626689682711528 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 18/20 (16 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00001 | RUNNING    | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551173 |                    5 |\n",
            "| train_model_2cb17_00002 | PENDING    |                   |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:34,384\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:36,373\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:38,340\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:40 (running for 00:00:53.86)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5534939836327246 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 18/20 (16 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00001 | RUNNING    | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.528699 |                    8 |\n",
            "| train_model_2cb17_00002 | PENDING    |                   |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:40,294\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:42,258\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55576)\u001b[0m 2023-01-30 19:28:44,212\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:50 (running for 00:01:03.63)\n",
            "Memory usage on this node: 3.8/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5534939836327246 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 19/20 (16 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00002 | RUNNING    | 172.28.0.12:55792 |           32 |            48 |  0.225601 |          |                      |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:28:54,756\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:28:56 (running for 00:01:10.31)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5534939836327246 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 19/20 (16 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00002 | RUNNING    | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.686676 |                    2 |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:28:56,745\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:28:58,706\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:00,674\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:02 (running for 00:01:16.24)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5534939836327246 | Iter 3.000: -0.6028687213362709\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 19/20 (16 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00002 | RUNNING    | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.558332 |                    5 |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:02,682\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:04,642\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:06,580\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:08 (running for 00:01:22.10)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6028687213362709\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 19/20 (16 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00002 | RUNNING    | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.529385 |                    8 |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:08,538\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:10,489\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:12,444\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:14 (running for 00:01:27.94)\n",
            "Memory usage on this node: 5.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6028687213362709\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 19/20 (16 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00002 | RUNNING    | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.54031  |                   11 |\n",
            "| train_model_2cb17_00003 | PENDING    |                   |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:14,379\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=55792)\u001b[0m 2023-01-30 19:29:16,309\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:22 (running for 00:01:35.64)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6028687213362709\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (16 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00003 | RUNNING    | 172.28.0.12:56028 |           64 |            32 |  0.112693 |          |                      |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 9.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 8.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.6 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 18 | output_layer                      | Linear                           | 561   \n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 70.8 K    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 70.8 K    Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 0.566     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 2023-01-30 19:29:26,242\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:27 (running for 00:01:41.07)\n",
            "Memory usage on this node: 5.5/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6028687213362709\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (16 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00003 | RUNNING    | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.706975 |                    2 |\n",
            "| train_model_2cb17_00004 | PENDING    |                   |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 2023-01-30 19:29:27,501\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56028)\u001b[0m 2023-01-30 19:29:28,767\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:34 (running for 00:01:47.66)\n",
            "Memory usage on this node: 4.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (15 PENDING, 1 RUNNING, 4 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00004 | RUNNING    | 172.28.0.12:56165 |          128 |            32 |  0.149138 |          |                      |\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 9.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 8.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.6 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 18 | output_layer                      | Linear                           | 561   \n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 70.8 K    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 70.8 K    Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 0.566     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 2023-01-30 19:29:37,548\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 2023-01-30 19:29:38,560\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:39 (running for 00:01:53.13)\n",
            "Memory usage on this node: 5.5/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (15 PENDING, 5 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00005 | PENDING    |                   |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56165)\u001b[0m 2023-01-30 19:29:39,559\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:45 (running for 00:01:58.64)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (14 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00005 | RUNNING    | 172.28.0.12:56295 |           32 |            48 |  0.291225 |          |                      |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:29:49,563\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:51 (running for 00:02:05.10)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (14 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00005 | RUNNING    | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.6906   |                    2 |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:29:51,534\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:29:53,470\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:29:55,388\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:29:57 (running for 00:02:10.88)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (14 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00005 | RUNNING    | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.557902 |                    5 |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:29:57,319\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:29:59,255\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:30:01,185\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:03 (running for 00:02:16.76)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (14 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00005 | RUNNING    | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.527668 |                    8 |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:30:03,200\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:30:05,159\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:30:07,097\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:09 (running for 00:02:22.60)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (14 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00005 | RUNNING    | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | PENDING    |                   |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56295)\u001b[0m 2023-01-30 19:30:09,042\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:15 (running for 00:02:28.66)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (13 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00006 | RUNNING    | 172.28.0.12:56520 |           64 |            48 |  0.270748 |          |                      |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 2023-01-30 19:30:19,248\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:20 (running for 00:02:34.26)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6123865307506359\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (13 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00006 | RUNNING    | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.759773 |                    2 |\n",
            "| train_model_2cb17_00007 | PENDING    |                   |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 2023-01-30 19:30:20,702\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56520)\u001b[0m 2023-01-30 19:30:22,159\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:28 (running for 00:02:41.66)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (12 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00007 | RUNNING    | 172.28.0.12:56663 |          128 |            64 |  0.275324 |          |                      |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 16.2 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 14.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 18 | output_layer                      | Linear                           | 1.1 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 238 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 238 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 1.908     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 2023-01-30 19:30:32,007\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:33 (running for 00:02:46.94)\n",
            "Memory usage on this node: 5.5/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (12 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00007 | RUNNING    | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.93231  |                    2 |\n",
            "| train_model_2cb17_00008 | PENDING    |                   |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 2023-01-30 19:30:33,375\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56663)\u001b[0m 2023-01-30 19:30:34,732\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:40 (running for 00:02:53.70)\n",
            "Memory usage on this node: 4.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.655447486767263\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (11 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00008 | RUNNING    | 172.28.0.12:56801 |           32 |            32 |  0.109133 |          |                      |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 9.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 8.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.6 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 18 | output_layer                      | Linear                           | 561   \n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 70.8 K    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 70.8 K    Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 0.566     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 2023-01-30 19:30:44,437\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:46 (running for 00:02:59.80)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.655447486767263\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (11 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00008 | RUNNING    | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.646982 |                    2 |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 2023-01-30 19:30:46,242\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 2023-01-30 19:30:47,992\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 2023-01-30 19:30:49,736\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:51 (running for 00:03:05.04)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6364470690502947\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (11 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00008 | RUNNING    | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.575998 |                    5 |\n",
            "| train_model_2cb17_00009 | PENDING    |                   |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 2023-01-30 19:30:51,476\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56801)\u001b[0m 2023-01-30 19:30:53,226\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:30:59 (running for 00:03:12.66)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6364470690502947\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (10 PENDING, 1 RUNNING, 9 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00009 | RUNNING    | 172.28.0.12:56972 |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 9.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 8.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.6 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 18 | output_layer                      | Linear                           | 561   \n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 70.8 K    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 70.8 K    Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 0.566     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m   rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:04 (running for 00:03:17.67)\n",
            "Memory usage on this node: 5.3/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6364470690502947\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (10 PENDING, 1 RUNNING, 9 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00009 | RUNNING    | 172.28.0.12:56972 |           16 |            32 |  0.18789  |          |                      |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:04,446\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:07,199\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:09,965\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:09 (running for 00:03:23.54)\n",
            "Memory usage on this node: 5.3/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (10 PENDING, 1 RUNNING, 9 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00009 | RUNNING    | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.578444 |                    3 |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:12,731\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:15 (running for 00:03:29.06)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5458145425030996 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (10 PENDING, 1 RUNNING, 9 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00009 | RUNNING    | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.569078 |                    5 |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:15,488\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:18,254\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:21 (running for 00:03:34.58)\n",
            "Memory usage on this node: 5.3/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (10 PENDING, 1 RUNNING, 9 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00009 | RUNNING    | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.535243 |                    7 |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:21,013\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:23,785\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:26 (running for 00:03:40.09)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (10 PENDING, 1 RUNNING, 9 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00009 | RUNNING    | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | PENDING    |                   |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=56972)\u001b[0m 2023-01-30 19:31:26,519\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:32 (running for 00:03:45.69)\n",
            "Memory usage on this node: 4.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (9 PENDING, 1 RUNNING, 10 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00010 | RUNNING    | 172.28.0.12:57209 |           32 |            48 |  0.130921 |          |                      |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:36,526\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:38 (running for 00:03:52.07)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (9 PENDING, 1 RUNNING, 10 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00010 | RUNNING    | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.671862 |                    2 |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:38,507\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:40,463\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:42,403\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:44 (running for 00:03:57.92)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.545066770748698 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (9 PENDING, 1 RUNNING, 10 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00010 | RUNNING    | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.552428 |                    5 |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:44,354\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:46,302\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:48,235\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:50 (running for 00:04:03.74)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (9 PENDING, 1 RUNNING, 10 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00010 | RUNNING    | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.527541 |                    8 |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:50,180\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:52,136\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:54,088\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:31:56 (running for 00:04:09.62)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (9 PENDING, 1 RUNNING, 10 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00010 | RUNNING    | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | PENDING    |                   |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57209)\u001b[0m 2023-01-30 19:31:56,054\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:01 (running for 00:04:14.68)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (8 PENDING, 1 RUNNING, 11 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00011 | RUNNING    | 172.28.0.12:57425 |          128 |            64 |  0.24035  |          |                      |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 16.2 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 14.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 18 | output_layer                      | Linear                           | 1.1 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 238 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 238 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 1.908     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 2023-01-30 19:32:05,067\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:06 (running for 00:04:20.02)\n",
            "Memory usage on this node: 5.5/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (8 PENDING, 1 RUNNING, 11 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00011 | RUNNING    | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.907433 |                    2 |\n",
            "| train_model_2cb17_00012 | PENDING    |                   |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 2023-01-30 19:32:06,453\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57425)\u001b[0m 2023-01-30 19:32:07,818\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:13 (running for 00:04:26.68)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (7 PENDING, 1 RUNNING, 12 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00012 | RUNNING    | 172.28.0.12:57563 |           32 |            48 |  0.278062 |          |                      |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 2023-01-30 19:32:17,636\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:19 (running for 00:04:33.20)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (7 PENDING, 1 RUNNING, 12 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00012 | RUNNING    | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.694765 |                    2 |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 2023-01-30 19:32:19,633\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 2023-01-30 19:32:21,619\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 2023-01-30 19:32:23,608\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:25 (running for 00:04:39.15)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (7 PENDING, 1 RUNNING, 12 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00012 | RUNNING    | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.552765 |                    5 |\n",
            "| train_model_2cb17_00013 | PENDING    |                   |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 2023-01-30 19:32:25,592\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57563)\u001b[0m 2023-01-30 19:32:27,564\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:33 (running for 00:04:46.68)\n",
            "Memory usage on this node: 4.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5446928121050276 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (6 PENDING, 1 RUNNING, 13 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00013 | RUNNING    | 172.28.0.12:57741 |           64 |            48 |  0.166128 |          |                      |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 2023-01-30 19:32:37,152\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:38 (running for 00:04:52.17)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5446928121050276 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (6 PENDING, 1 RUNNING, 13 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00013 | RUNNING    | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.723167 |                    2 |\n",
            "| train_model_2cb17_00014 | PENDING    |                   |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 2023-01-30 19:32:38,608\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57741)\u001b[0m 2023-01-30 19:32:40,046\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:45 (running for 00:04:58.68)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5446928121050276 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (5 PENDING, 1 RUNNING, 14 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00014 | RUNNING    | 172.28.0.12:57878 |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 16.2 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 14.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 18 | output_layer                      | Linear                           | 1.1 K \n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 238 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 238 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 1.908     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m   rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:50 (running for 00:05:03.75)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5446928121050276 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (5 PENDING, 1 RUNNING, 14 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00014 | RUNNING    | 172.28.0.12:57878 |           16 |            64 |  0.104861 |          |                      |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:32:50,855\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:32:54,084\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:32:57 (running for 00:05:10.85)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5446928121050276 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (5 PENDING, 1 RUNNING, 14 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00014 | RUNNING    | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.54921  |                    3 |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:32:57,282\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:33:00,488\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:03 (running for 00:05:17.32)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5446928121050276 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (5 PENDING, 1 RUNNING, 14 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00014 | RUNNING    | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.554973 |                    5 |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:33:03,747\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:33:06,950\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:10 (running for 00:05:23.74)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00000 with SMAPE=0.5226664674515771 and parameters={'batch_size': 32, 'hidden_size': 64, 'dropout': 0.12794087613403066}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (5 PENDING, 1 RUNNING, 14 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00014 | RUNNING    | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.536689 |                    7 |\n",
            "| train_model_2cb17_00015 | PENDING    |                   |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:33:10,167\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=57878)\u001b[0m 2023-01-30 19:33:13,376\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:19 (running for 00:05:32.68)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (4 PENDING, 1 RUNNING, 15 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00015 | RUNNING    | 172.28.0.12:58118 |           32 |            32 |  0.299592 |          |                      |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 9.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 8.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.6 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 18 | output_layer                      | Linear                           | 561   \n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 70.8 K    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 70.8 K    Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 0.566     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 2023-01-30 19:33:23,572\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:25 (running for 00:05:38.90)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (4 PENDING, 1 RUNNING, 15 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00015 | RUNNING    | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.673552 |                    2 |\n",
            "| train_model_2cb17_00016 | PENDING    |                   |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 2023-01-30 19:33:25,335\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58118)\u001b[0m 2023-01-30 19:33:27,093\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:32 (running for 00:05:45.68)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (3 PENDING, 1 RUNNING, 16 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00016 | RUNNING    | 172.28.0.12:58258 |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 11.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 18.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 11.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 5.9 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 9.5 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 4.8 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 18 | output_layer                      | Linear                           | 833   \n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 141 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 141 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 1.136     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m   rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:37 (running for 00:05:51.06)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (3 PENDING, 1 RUNNING, 16 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00016 | RUNNING    | 172.28.0.12:58258 |           16 |            48 |  0.155627 |          |                      |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:37,956\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:40,934\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:43,897\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:43 (running for 00:05:57.47)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (3 PENDING, 1 RUNNING, 16 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00016 | RUNNING    | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.553422 |                    3 |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:46,877\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:49 (running for 00:06:03.40)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5443189989942965 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (3 PENDING, 1 RUNNING, 16 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00016 | RUNNING    | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.537272 |                    5 |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:49,824\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:52,804\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:33:55 (running for 00:06:09.31)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (3 PENDING, 1 RUNNING, 16 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00016 | RUNNING    | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.543934 |                    7 |\n",
            "| train_model_2cb17_00017 | PENDING    |                   |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:55,744\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58258)\u001b[0m 2023-01-30 19:33:58,714\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:34:04 (running for 00:06:17.69)\n",
            "Memory usage on this node: 4.0/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (2 PENDING, 1 RUNNING, 17 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00017 | RUNNING    | 172.28.0.12:58485 |           32 |            32 |  0.291603 |          |                      |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "| train_model_2cb17_00016 | TERMINATED | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.541078 |                    8 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 9.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 8.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.6 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 18 | output_layer                      | Linear                           | 561   \n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 70.8 K    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 70.8 K    Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 0.566     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 2023-01-30 19:34:08,708\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:34:10 (running for 00:06:24.07)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6219043401650008\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (2 PENDING, 1 RUNNING, 17 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00017 | RUNNING    | 172.28.0.12:58485 |           32 |            32 |  0.291603 | 0.670541 |                    2 |\n",
            "| train_model_2cb17_00018 | PENDING    |                   |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "| train_model_2cb17_00016 | TERMINATED | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.541078 |                    8 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 2023-01-30 19:34:10,508\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58485)\u001b[0m 2023-01-30 19:34:12,315\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:34:18 (running for 00:06:31.75)\n",
            "Memory usage on this node: 4.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=10\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (1 PENDING, 1 RUNNING, 18 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00018 | RUNNING    | 172.28.0.12:58634 |          128 |            64 |  0.110607 |          |                      |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "| train_model_2cb17_00016 | TERMINATED | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.541078 |                    8 |\n",
            "| train_model_2cb17_00017 | TERMINATED | 172.28.0.12:58485 |           32 |            32 |  0.291603 | 0.654403 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 16.2 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 14.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 33.3 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 18 | output_layer                      | Linear                           | 1.1 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 238 K     Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 238 K     Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 1.908     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 2023-01-30 19:34:22,072\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:34:23 (running for 00:06:37.04)\n",
            "Memory usage on this node: 5.5/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=10\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6291757046076478\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (1 PENDING, 1 RUNNING, 18 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00018 | RUNNING    | 172.28.0.12:58634 |          128 |            64 |  0.110607 | 0.789657 |                    2 |\n",
            "| train_model_2cb17_00019 | PENDING    |                   |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "| train_model_2cb17_00016 | TERMINATED | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.541078 |                    8 |\n",
            "| train_model_2cb17_00017 | TERMINATED | 172.28.0.12:58485 |           32 |            32 |  0.291603 | 0.654403 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 2023-01-30 19:34:23,474\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58634)\u001b[0m 2023-01-30 19:34:24,855\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:34:30 (running for 00:06:43.77)\n",
            "Memory usage on this node: 4.1/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=11\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6364470690502947\n",
            "Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (1 RUNNING, 19 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00019 | RUNNING    | 172.28.0.12:58771 |          128 |            32 |  0.239479 |          |                      |\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "| train_model_2cb17_00016 | TERMINATED | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.541078 |                    8 |\n",
            "| train_model_2cb17_00017 | TERMINATED | 172.28.0.12:58485 |           32 |            32 |  0.291603 | 0.654403 |                    3 |\n",
            "| train_model_2cb17_00018 | TERMINATED | 172.28.0.12:58634 |          128 |            64 |  0.110607 | 0.687745 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m GPU available: True (cuda), used: False\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m    | Name                              | Type                             | Params\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 0  | train_metrics                     | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 1  | val_metrics                       | MetricCollection                 | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 4  | encoder_vsn                       | _VariableSelectionNetwork        | 9.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 5  | decoder_vsn                       | _VariableSelectionNetwork        | 8.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 6  | static_context_grn                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 9  | static_context_enrichment         | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 10 | lstm_encoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 11 | lstm_decoder                      | LSTM                             | 8.4 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 12 | post_lstm_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.6 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 15 | post_attn_gan                     | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 16 | feed_forward_block                | _GatedResidualNetwork            | 4.3 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 17 | pre_output_gan                    | _GateAddNorm                     | 2.2 K \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 18 | output_layer                      | Linear                           | 561   \n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m ----------------------------------------------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 70.8 K    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 70.8 K    Total params\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 0.566     Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_SymmetricMeanAbsolutePercentageError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m /usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m   rank_zero_warn(\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 2023-01-30 19:34:33,669\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 2023-01-30 19:34:34,690\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "\u001b[2m\u001b[36m(train_model pid=58771)\u001b[0m 2023-01-30 19:34:35,706\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
            "2023-01-30 19:34:35,838\tINFO tune.py:762 -- Total run time: 409.67 seconds (409.28 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-30 19:34:35 (running for 00:06:49.28)\n",
            "Memory usage on this node: 5.4/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=12\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6454252438354487\n",
            "Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (20 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "| train_model_2cb17_00016 | TERMINATED | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.541078 |                    8 |\n",
            "| train_model_2cb17_00017 | TERMINATED | 172.28.0.12:58485 |           32 |            32 |  0.291603 | 0.654403 |                    3 |\n",
            "| train_model_2cb17_00018 | TERMINATED | 172.28.0.12:58634 |          128 |            64 |  0.110607 | 0.687745 |                    3 |\n",
            "| train_model_2cb17_00019 | TERMINATED | 172.28.0.12:58771 |          128 |            32 |  0.239479 | 0.837275 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-01-30 19:34:35 (running for 00:06:49.29)\n",
            "Memory usage on this node: 5.3/83.5 GiB \n",
            "Using AsyncHyperBand: num_stopped=12\n",
            "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.5325960201874448 | Iter 6.000: -0.5441701035779936 | Iter 3.000: -0.6454252438354487\n",
            "Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/49.84 GiB heap, 0.0/24.92 GiB objects\n",
            "Current best trial: 2cb17_00014 with SMAPE=0.504148006038673 and parameters={'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n",
            "Result logdir: /root/ray_results/tune_darts\n",
            "Number of trials: 20/20 (20 TERMINATED)\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "| Trial name              | status     | loc               |   batch_size |   hidden_size |   dropout |    SMAPE |   training_iteration |\n",
            "|-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------|\n",
            "| train_model_2cb17_00000 | TERMINATED | 172.28.0.12:55343 |           32 |            64 |  0.127941 | 0.522666 |                   10 |\n",
            "| train_model_2cb17_00001 | TERMINATED | 172.28.0.12:55576 |           32 |            48 |  0.241223 | 0.551715 |                   10 |\n",
            "| train_model_2cb17_00002 | TERMINATED | 172.28.0.12:55792 |           32 |            48 |  0.225601 | 0.532596 |                   12 |\n",
            "| train_model_2cb17_00003 | TERMINATED | 172.28.0.12:56028 |           64 |            32 |  0.112693 | 0.689665 |                    3 |\n",
            "| train_model_2cb17_00004 | TERMINATED | 172.28.0.12:56165 |          128 |            32 |  0.149138 | 0.783591 |                    3 |\n",
            "| train_model_2cb17_00005 | TERMINATED | 172.28.0.12:56295 |           32 |            48 |  0.291225 | 0.539846 |                   11 |\n",
            "| train_model_2cb17_00006 | TERMINATED | 172.28.0.12:56520 |           64 |            48 |  0.270748 | 0.688991 |                    3 |\n",
            "| train_model_2cb17_00007 | TERMINATED | 172.28.0.12:56663 |          128 |            64 |  0.275324 | 0.789988 |                    3 |\n",
            "| train_model_2cb17_00008 | TERMINATED | 172.28.0.12:56801 |           32 |            32 |  0.109133 | 0.556069 |                    6 |\n",
            "| train_model_2cb17_00009 | TERMINATED | 172.28.0.12:56972 |           16 |            32 |  0.18789  | 0.526454 |                    9 |\n",
            "| train_model_2cb17_00010 | TERMINATED | 172.28.0.12:57209 |           32 |            48 |  0.130921 | 0.534799 |                   11 |\n",
            "| train_model_2cb17_00011 | TERMINATED | 172.28.0.12:57425 |          128 |            64 |  0.24035  | 0.754726 |                    3 |\n",
            "| train_model_2cb17_00012 | TERMINATED | 172.28.0.12:57563 |           32 |            48 |  0.278062 | 0.545067 |                    6 |\n",
            "| train_model_2cb17_00013 | TERMINATED | 172.28.0.12:57741 |           64 |            48 |  0.166128 | 0.669737 |                    3 |\n",
            "| train_model_2cb17_00014 | TERMINATED | 172.28.0.12:57878 |           16 |            64 |  0.104861 | 0.504148 |                    8 |\n",
            "| train_model_2cb17_00015 | TERMINATED | 172.28.0.12:58118 |           32 |            32 |  0.299592 | 0.654586 |                    3 |\n",
            "| train_model_2cb17_00016 | TERMINATED | 172.28.0.12:58258 |           16 |            48 |  0.155627 | 0.541078 |                    8 |\n",
            "| train_model_2cb17_00017 | TERMINATED | 172.28.0.12:58485 |           32 |            32 |  0.291603 | 0.654403 |                    3 |\n",
            "| train_model_2cb17_00018 | TERMINATED | 172.28.0.12:58634 |          128 |            64 |  0.110607 | 0.687745 |                    3 |\n",
            "| train_model_2cb17_00019 | TERMINATED | 172.28.0.12:58771 |          128 |            32 |  0.239479 | 0.837275 |                    3 |\n",
            "+-------------------------+------------+-------------------+--------------+---------------+-----------+----------+----------------------+\n",
            "\n",
            "\n",
            "Best hyperparameters found were:  {'batch_size': 16, 'hidden_size': 64, 'dropout': 0.1048610286049492}\n"
          ]
        }
      ],
      "source": [
        "my_stopper = EarlyStopping(\n",
        "    monitor=\"val_SymmetricMeanAbsolutePercentageError\",\n",
        "    patience=5,\n",
        "    min_delta=0.05,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "# set up ray tune callback\n",
        "tune_callback = TuneReportCallback(\n",
        "    {\n",
        "        \"loss\": \"val_Loss\",\n",
        "        \"SMAPE\": \"val_SymmetricMeanAbsolutePercentageError\",\n",
        "    },\n",
        "    on=\"validation_end\",\n",
        ")\n",
        "\n",
        "# define the hyperparameter space\n",
        "config = {\n",
        "    #\"input_chunk_length\": tune.choice([7, 28, 30]),\n",
        "    #\"output_chunk_length\": tune.choice([1, 7]),\n",
        "    \"batch_size\": tune.choice([16, 32, 64, 128]),\n",
        "    \"hidden_size\": tune.choice([32, 48, 64]),\n",
        "    \"dropout\": tune.uniform(0.1, 0.3),\n",
        "}\n",
        "\n",
        "reporter = CLIReporter(\n",
        "    parameter_columns=list(config.keys()),\n",
        "    metric_columns=[\"loss\", \"SMAPE\", \"training_iteration\"],\n",
        ")\n",
        "\n",
        "resources_per_trial = {\"cpu\": 12, \"gpu\": 1}\n",
        "\n",
        "# the number of combinations to try\n",
        "num_samples = 20\n",
        "\n",
        "scheduler = ASHAScheduler(max_t=1000, grace_period=3, reduction_factor=2)\n",
        "\n",
        "train_fn_with_parameters = tune.with_parameters(\n",
        "    train_model, callbacks=[my_stopper, tune_callback], train=train, val=val, covariates=covariates\n",
        ")\n",
        "\n",
        "# optimize hyperparameters by minimizing the SMAPE on the validation set\n",
        "analysis = tune.run(\n",
        "    train_fn_with_parameters,\n",
        "    resources_per_trial=resources_per_trial,\n",
        "    # Using a metric instead of loss allows for\n",
        "    # comparison between different likelihood or loss functions.\n",
        "    metric=\"SMAPE\",  # any value in TuneReportCallback.\n",
        "    mode=\"min\",\n",
        "    config=config,\n",
        "    num_samples=num_samples,\n",
        "    scheduler=scheduler,\n",
        "    progress_reporter=reporter,\n",
        "    name=\"tune_darts\",\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters found were: \", analysis.best_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision('medium')"
      ],
      "metadata": {
        "id": "i6yVigHux-0Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_num_list[64]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMZbmhsz9gBR",
        "outputId": "778c85a4-0f33-4961-b53a-8ebc4ef8a769"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "861"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlUyy17Rqcc2",
        "outputId": "b7272cb0-edff-45fc-cd4b-f681e2ae0d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/35 [00:00<?, ?it/s]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "  3%|▎         | 1/35 [19:44<11:11:03, 1184.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "968 9.24200405313524 0.12143650325133076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "  6%|▌         | 2/35 [39:19<10:48:28, 1179.06s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "978 10.190973515163423 0.15296954963507944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "  9%|▊         | 3/35 [58:36<10:23:18, 1168.71s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "996 9.639201933416272 0.12730881446766235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 11%|█▏        | 4/35 [1:17:50<10:00:53, 1163.00s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1006 9.667770384231448 0.12182260035817427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 14%|█▍        | 5/35 [1:37:05<9:40:07, 1160.26s/it] INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1055 5.687080884707123 0.07136595291771304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 17%|█▋        | 6/35 [1:56:24<9:20:37, 1159.90s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31 6.94265911128481 0.07852335255173476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 20%|██        | 7/35 [2:15:39<9:00:27, 1158.12s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56 5.540403547475191 0.07455703865190172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 23%|██▎       | 8/35 [2:35:25<8:45:13, 1167.16s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75 6.966519192810439 0.08145062506344318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 26%|██▌       | 9/35 [2:54:40<8:24:03, 1163.22s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 6.89486972225043 0.08648829386283235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 29%|██▊       | 10/35 [3:14:01<8:04:27, 1162.69s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94 7.580922482379647 0.09505062702730273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 31%|███▏      | 11/35 [3:33:12<7:43:32, 1158.84s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "154 6.056451379908754 0.07194664658447839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 34%|███▍      | 12/35 [3:52:22<7:23:13, 1156.25s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "261 6.9194475145308605 0.09943071465551911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 37%|███▋      | 13/35 [4:11:39<7:04:01, 1156.41s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291 5.849714590441654 0.06850766598424839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 40%|████      | 14/35 [4:30:55<6:44:46, 1156.51s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350 6.81376039056464 0.08251441474771072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 43%|████▎     | 15/35 [4:50:09<6:25:10, 1155.52s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "356 10.548933663256236 0.16288516780470827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 46%|████▌     | 16/35 [5:09:21<6:05:39, 1154.70s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "372 10.966105603811153 0.12059016411654451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 49%|████▊     | 17/35 [5:28:30<5:45:52, 1152.91s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "424 6.335195893222536 0.08283311239072973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 51%|█████▏    | 18/35 [5:47:41<5:26:28, 1152.28s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "493 8.579432932837205 0.11318703643252376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 54%|█████▍    | 19/35 [6:06:52<5:07:11, 1151.94s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521 5.799764469206979 0.07226459393015167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 57%|█████▋    | 20/35 [6:26:05<4:48:01, 1152.11s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "525 6.181171364100386 0.07286589612147873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 60%|██████    | 21/35 [6:45:21<4:29:06, 1153.34s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "578 8.302687036265612 0.09285072277403787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 63%|██████▎   | 22/35 [7:04:48<4:10:45, 1157.34s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "585 8.80354111507425 0.09906133850352457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 66%|██████▌   | 23/35 [7:24:02<3:51:19, 1156.62s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "601 9.485758032481522 0.12389226292850669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 69%|██████▊   | 24/35 [7:43:16<3:31:52, 1155.71s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "605 10.138226053478308 0.12470240500157298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 71%|███████▏  | 25/35 [8:02:49<3:13:29, 1160.91s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "723 6.408409344393795 0.08150267573704122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 74%|███████▍  | 26/35 [8:22:20<2:54:34, 1163.86s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "740 8.260845163183037 0.09548901771411168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 77%|███████▋  | 27/35 [8:41:52<2:35:32, 1166.50s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "803 8.332505536217786 0.0920983628487456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 80%|████████  | 28/35 [9:01:23<2:16:13, 1167.69s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "850 12.72740597480983 0.13567302567857162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 83%|████████▎ | 29/35 [9:20:54<1:56:51, 1168.62s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "865 6.78813450361613 0.08147991856951721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 86%|████████▌ | 30/35 [9:40:12<1:37:07, 1165.55s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "940 6.644291070340225 0.07649019561845889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 89%|████████▊ | 31/35 [9:59:56<1:18:03, 1170.98s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "960 5.670291494800022 0.06956217736334601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 91%|█████████▏| 32/35 [10:20:02<59:04, 1181.51s/it] INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1010 9.617648728671787 0.1086632316596593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 94%|█████████▍| 33/35 [10:40:05<39:35, 1187.96s/it]INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1046 6.846393681576143 0.08918072868779373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            " 97%|█████████▋| 34/35 [11:00:07<19:52, 1192.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1089 7.866645663111199 0.09115110245040546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=300` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "100%|██████████| 35/35 [11:19:26<00:00, 1164.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1101 5.91791156003411 0.07550253919020407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "smape_list = []\n",
        "rmspe_list = []\n",
        "\n",
        "for num in tqdm(store_num_list[65:]):\n",
        "    train_transformed, test_transformed, covariates = get_sample(num)\n",
        "    tft_model = TFTModel(\n",
        "        input_chunk_length=28,\n",
        "        output_chunk_length=7,\n",
        "        hidden_size=64,\n",
        "        lstm_layers=1,\n",
        "        num_attention_heads=4,\n",
        "        dropout= 0.1,\n",
        "        batch_size=16,\n",
        "        n_epochs=300,\n",
        "        add_relative_index=False,\n",
        "        add_encoders=None,\n",
        "        likelihood=QuantileRegression(\n",
        "                quantiles=quantiles\n",
        "        ),  \n",
        "        random_state=42, \n",
        "        pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0], \"enable_progress_bar\": False}\n",
        "    )\n",
        "    smape_score, rmspe_score = evaluate_covariates_model(tft_model, train_transformed, test_transformed, covariates )\n",
        "    rmspe_list.append(rmspe_score)\n",
        "    smape_list.append(smape_score)\n",
        "    print(num, smape_score, rmspe_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "E2D2mPpeqcc2",
        "outputId": "ce1d5509-794b-4067-fe14-01e586b1dc94"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1c904a33e717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mres_h1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_num_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mres_h1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \"\"\"\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (100) does not match length of index (35)"
          ]
        }
      ],
      "source": [
        "res_h1 = pd.DataFrame(\n",
        "    {\n",
        "        'rmspe' : rmspe_list,\n",
        "        'smape' : smape_list,\n",
        "    }\n",
        ")\n",
        "res_h1['Store'] = store_num_list\n",
        "res_h1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ox2jpmqcc3"
      },
      "outputs": [],
      "source": [
        "result = res_h1.set_index('Store')\n",
        "result = pd.merge(result, store[['Store','StoreType']], left_on='Store', right_on='Store')\n",
        "result.to_csv('/content/drive/MyDrive/tft_hyper_result.csv')\n",
        "result.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dd7ewhttyMFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "darts",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1 (default, Mar  2 2020, 13:06:26) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5e4eb80f0a2947b2b4d080aa13c6d15fa0bc013de0be3eb24296c75a14756372"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}