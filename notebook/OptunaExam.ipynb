{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:06:29 (running for 00:00:00.20)\n",
      "Memory usage on this node: 16.0/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+\n",
      "| Trial name              | status   | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+--------------+------------|\n",
      "| train_model_25b11_00000 | RUNNING  | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   |\n",
      "| train_model_25b11_00001 | PENDING  |                 |           16 |            4 |          128 | 0.0723749  |\n",
      "| train_model_25b11_00002 | PENDING  |                 |          128 |            2 |           32 | 0.199994   |\n",
      "| train_model_25b11_00003 | PENDING  |                 |           32 |            1 |           32 | 0.0429654  |\n",
      "| train_model_25b11_00004 | PENDING  |                 |           64 |            2 |           32 | 0.145425   |\n",
      "| train_model_25b11_00005 | PENDING  |                 |           16 |            1 |           32 | 0.142135   |\n",
      "| train_model_25b11_00006 | PENDING  |                 |          128 |            1 |           64 | 0.0992416  |\n",
      "| train_model_25b11_00007 | PENDING  |                 |           32 |            1 |          128 | 0.00862542 |\n",
      "| train_model_25b11_00008 | PENDING  |                 |          128 |            3 |           32 | 0.121705   |\n",
      "| train_model_25b11_00009 | PENDING  |                 |          128 |            2 |          128 | 0.0280488  |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 3 | stacks        | ModuleList       | 19.8 M\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 19.8 M    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 19.8 M    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 158.640   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">    MAPE</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname   </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_25b11_00000</td><td style=\"text-align: right;\">0.135828</td><td>2023-01-30_19-06-47</td><td>True  </td><td>                </td><td>16dc52cfafb340f6b61c066d3fa31aa5</td><td>Tensortorch</td><td style=\"text-align: right;\">                        19</td><td>127.0.0.1</td><td style=\"text-align: right;\">29528</td><td style=\"text-align: right;\">            13.0008 </td><td style=\"text-align: right;\">          0.654763</td><td style=\"text-align: right;\">      13.0008 </td><td style=\"text-align: right;\"> 1675076807</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  19</td><td>25b11_00000</td><td style=\"text-align: right;\">   0.00414515</td></tr>\n",
       "<tr><td>train_model_25b11_00001</td><td style=\"text-align: right;\">1.32436 </td><td>2023-01-30_19-07-25</td><td>True  </td><td>                </td><td>d76c7e79ecd5404e922023eed73f9778</td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">24532</td><td style=\"text-align: right;\">            31.9876 </td><td style=\"text-align: right;\">         10.0011  </td><td style=\"text-align: right;\">      31.9876 </td><td style=\"text-align: right;\"> 1675076845</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>25b11_00001</td><td style=\"text-align: right;\">   0.00511074</td></tr>\n",
       "<tr><td>train_model_25b11_00002</td><td style=\"text-align: right;\">0.731292</td><td>2023-01-30_19-07-33</td><td>True  </td><td>                </td><td>71508ac2f2a64c8e933d9335c0428cf3</td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 5548</td><td style=\"text-align: right;\">             3.04695</td><td style=\"text-align: right;\">          0.454168</td><td style=\"text-align: right;\">       3.04695</td><td style=\"text-align: right;\"> 1675076853</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>25b11_00002</td><td style=\"text-align: right;\">   0.00501823</td></tr>\n",
       "<tr><td>train_model_25b11_00003</td><td style=\"text-align: right;\">0.148242</td><td>2023-01-30_19-07-46</td><td>True  </td><td>                </td><td>eedebf5888f04aec929486765f6fffa0</td><td>Tensortorch</td><td style=\"text-align: right;\">                        16</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 1812</td><td style=\"text-align: right;\">             7.3136 </td><td style=\"text-align: right;\">          0.409638</td><td style=\"text-align: right;\">       7.3136 </td><td style=\"text-align: right;\"> 1675076866</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  16</td><td>25b11_00003</td><td style=\"text-align: right;\">   0.0045054 </td></tr>\n",
       "<tr><td>train_model_25b11_00004</td><td style=\"text-align: right;\">0.396265</td><td>2023-01-30_19-07-56</td><td>True  </td><td>                </td><td>1a0910ec2ce44660ac64076873a349d2</td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">30116</td><td style=\"text-align: right;\">             4.17662</td><td style=\"text-align: right;\">          0.630519</td><td style=\"text-align: right;\">       4.17662</td><td style=\"text-align: right;\"> 1675076876</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>25b11_00004</td><td style=\"text-align: right;\">   0.00511813</td></tr>\n",
       "<tr><td>train_model_25b11_00005</td><td style=\"text-align: right;\">0.141592</td><td>2023-01-30_19-08-11</td><td>True  </td><td>                </td><td>25f5ad96e9e0443184d560857d6bcd7f</td><td>Tensortorch</td><td style=\"text-align: right;\">                        14</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 8832</td><td style=\"text-align: right;\">             8.69273</td><td style=\"text-align: right;\">          0.594927</td><td style=\"text-align: right;\">       8.69273</td><td style=\"text-align: right;\"> 1675076891</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  14</td><td>25b11_00005</td><td style=\"text-align: right;\">   0.00357771</td></tr>\n",
       "<tr><td>train_model_25b11_00006</td><td style=\"text-align: right;\">0.749197</td><td>2023-01-30_19-08-18</td><td>True  </td><td>                </td><td>36ae72ae14784be18ccccdae85631393</td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 6084</td><td style=\"text-align: right;\">             1.64744</td><td style=\"text-align: right;\">          0.431654</td><td style=\"text-align: right;\">       1.64744</td><td style=\"text-align: right;\"> 1675076898</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>25b11_00006</td><td style=\"text-align: right;\">   0.00292468</td></tr>\n",
       "<tr><td>train_model_25b11_00007</td><td style=\"text-align: right;\">0.162758</td><td>2023-01-30_19-08-40</td><td>True  </td><td>                </td><td>b6e854fd6e014008ad19c357105d89c9</td><td>Tensortorch</td><td style=\"text-align: right;\">                        10</td><td>127.0.0.1</td><td style=\"text-align: right;\">15980</td><td style=\"text-align: right;\">            17.3867 </td><td style=\"text-align: right;\">          1.69241 </td><td style=\"text-align: right;\">      17.3867 </td><td style=\"text-align: right;\"> 1675076920</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>25b11_00007</td><td style=\"text-align: right;\">   0.00447035</td></tr>\n",
       "<tr><td>train_model_25b11_00008</td><td style=\"text-align: right;\">0.52124 </td><td>2023-01-30_19-08-51</td><td>True  </td><td>                </td><td>34f0eca718b246debbb4447ced8659b3</td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">11172</td><td style=\"text-align: right;\">             4.39312</td><td style=\"text-align: right;\">          0.656506</td><td style=\"text-align: right;\">       4.39312</td><td style=\"text-align: right;\"> 1675076931</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>25b11_00008</td><td style=\"text-align: right;\">   0.00453234</td></tr>\n",
       "<tr><td>train_model_25b11_00009</td><td style=\"text-align: right;\">0.9174  </td><td>2023-01-30_19-09-03</td><td>True  </td><td>                </td><td>87f8e885013d43589bdc3044a0c075c7</td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">29712</td><td style=\"text-align: right;\">             6.40906</td><td style=\"text-align: right;\">          1.83459 </td><td style=\"text-align: right;\">       6.40906</td><td style=\"text-align: right;\"> 1675076943</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>25b11_00009</td><td style=\"text-align: right;\">   0.00500107</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:06:35 (running for 00:00:06.18)\n",
      "Memory usage on this node: 17.1/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.9014418882324087 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status   | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00000 | RUNNING  | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.901442 |                    1 |\n",
      "| train_model_25b11_00001 | PENDING  |                 |           16 |            4 |          128 | 0.0723749  |          |                      |\n",
      "| train_model_25b11_00002 | PENDING  |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING  |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING  |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING  |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING  |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING  |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING  |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING  |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:35,825\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:36,485\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:37,131\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:37,772\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:38,423\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:39,076\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:39,749\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:40,423\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:06:41 (running for 00:00:11.44)\n",
      "Memory usage on this node: 17.0/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.30523278092261835 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status   | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00000 | RUNNING  | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.305233 |                    9 |\n",
      "| train_model_25b11_00001 | PENDING  |                 |           16 |            4 |          128 | 0.0723749  |          |                      |\n",
      "| train_model_25b11_00002 | PENDING  |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING  |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING  |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING  |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING  |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING  |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING  |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING  |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:41,083\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:41,751\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:42,411\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:43,074\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:43,743\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:44,412\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:45,082\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:45,761\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:06:46 (running for 00:00:16.78)\n",
      "Memory usage on this node: 16.9/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.1479540979916482 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status   | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00000 | RUNNING  | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.147954 |                   17 |\n",
      "| train_model_25b11_00001 | PENDING  |                 |           16 |            4 |          128 | 0.0723749  |          |                      |\n",
      "| train_model_25b11_00002 | PENDING  |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING  |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING  |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING  |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING  |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING  |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING  |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING  |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:46,424\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:47,084\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29528)\u001b[0m 2023-01-30 19:06:47,739\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:06:53 (running for 00:00:23.47)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00001 | RUNNING    | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  |          |                      |\n",
      "| train_model_25b11_00002 | PENDING    |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING    |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 3 | stacks        | ModuleList       | 105 M \n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 105 M     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 105 M     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 846.078   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:06:58 (running for 00:00:28.52)\n",
      "Memory usage on this node: 19.7/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00001 | RUNNING    | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  |          |                      |\n",
      "| train_model_25b11_00002 | PENDING    |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING    |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:03 (running for 00:00:33.59)\n",
      "Memory usage on this node: 19.7/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00001 | RUNNING    | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  |          |                      |\n",
      "| train_model_25b11_00002 | PENDING    |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING    |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 2023-01-30 19:07:05,090\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:10 (running for 00:00:40.47)\n",
      "Memory usage on this node: 19.7/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00001 | RUNNING    | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 5.47153  |                    1 |\n",
      "| train_model_25b11_00002 | PENDING    |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING    |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 2023-01-30 19:07:15,041\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:20 (running for 00:00:50.44)\n",
      "Memory usage on this node: 19.7/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00001 | RUNNING    | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 0.669588 |                    2 |\n",
      "| train_model_25b11_00002 | PENDING    |                 |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING    |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=24532)\u001b[0m 2023-01-30 19:07:25,042\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:25 (running for 00:00:55.67)\n",
      "Memory usage on this node: 17.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.9701083678554281\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00002 | RUNNING    | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   |          |                      |\n",
      "| train_model_25b11_00003 | PENDING    |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 3 | stacks        | ModuleList       | 13.2 M\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 13.2 M    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 13.2 M    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 105.760   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:31 (running for 00:01:01.52)\n",
      "Memory usage on this node: 16.7/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.9701083678554281\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00002 | RUNNING    | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.727804 |                    1 |\n",
      "| train_model_25b11_00003 | PENDING    |                 |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 2023-01-30 19:07:31,167\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 2023-01-30 19:07:31,616\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 2023-01-30 19:07:32,054\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 2023-01-30 19:07:32,501\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 2023-01-30 19:07:32,943\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=5548)\u001b[0m 2023-01-30 19:07:33,398\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:39 (running for 00:01:09.76)\n",
      "Memory usage on this node: 16.0/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.5427569689087941 | Iter 3.000: -0.6158535103774475\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00003 | RUNNING    | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  |          |                      |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 3 | stacks        | ModuleList       | 6.6 M \n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 6.6 M     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 6.6 M     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 52.880    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:40,081\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:40,492\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:40,903\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:41,324\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:41,726\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:42,143\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:42,557\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:42,965\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:43,472\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:43,918\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:44 (running for 00:01:14.78)\n",
      "Memory usage on this node: 16.5/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.21105670675590335 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5954685026756124\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00003 | RUNNING    | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.172792 |                   11 |\n",
      "| train_model_25b11_00004 | PENDING    |                 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:44,433\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:44,906\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:45,380\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:45,845\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:46,292\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1812)\u001b[0m 2023-01-30 19:07:46,701\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:52 (running for 00:01:22.82)\n",
      "Memory usage on this node: 16.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.17968257521399705 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5954685026756124\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00004 | RUNNING    | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   |          |                      |\n",
      "| train_model_25b11_00005 | PENDING    |                 |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 3 | stacks        | ModuleList       | 13.2 M\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 13.2 M    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 13.2 M    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 105.760   Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 2023-01-30 19:07:53,413\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 2023-01-30 19:07:54,056\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 2023-01-30 19:07:54,698\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 2023-01-30 19:07:55,334\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 2023-01-30 19:07:55,961\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30116)\u001b[0m 2023-01-30 19:07:56,592\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:07:57 (running for 00:01:27.83)\n",
      "Memory usage on this node: 15.8/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.17968257521399705 | Iter 6.000: -0.3752431640961541 | Iter 3.000: -0.5835992759377909\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00005 | RUNNING    | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   |          |                      |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 3 | stacks        | ModuleList       | 6.6 M \n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 6.6 M     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 6.6 M     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 52.880    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:03,298\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:03 (running for 00:01:33.68)\n",
      "Memory usage on this node: 16.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.17968257521399705 | Iter 6.000: -0.3752431640961541 | Iter 3.000: -0.5835992759377909\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00005 | RUNNING    | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.46119  |                    1 |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:03,906\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:04,511\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:05,102\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:05,699\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:06,290\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:06,893\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:07,498\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:08,098\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:08 (running for 00:01:39.05)\n",
      "Memory usage on this node: 16.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.17968257521399705 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5793413854557841\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00005 | RUNNING    | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.172393 |                   10 |\n",
      "| train_model_25b11_00006 | PENDING    |                 |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:08,699\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:09,304\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:09,898\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:10,496\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=8832)\u001b[0m 2023-01-30 19:08:11,091\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 3 | stacks        | ModuleList       | 13.2 M\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 13.2 M    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 13.2 M    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 105.760   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:16 (running for 00:01:46.97)\n",
      "Memory usage on this node: 16.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5793413854557841\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00006 | RUNNING    | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  |          |                      |\n",
      "| train_model_25b11_00007 | PENDING    |                 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 2023-01-30 19:08:17,174\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 2023-01-30 19:08:17,612\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6084)\u001b[0m 2023-01-30 19:08:18,043\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:23 (running for 00:01:53.98)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5835992759377909\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00007 | RUNNING    | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 |          |                      |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 3 | stacks        | ModuleList       | 26.4 M\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 26.4 M    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 26.4 M    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 211.519   Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:25,727\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:27,389\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:29 (running for 00:01:59.43)\n",
      "Memory usage on this node: 17.1/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5793413854557841\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00007 | RUNNING    | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 | 0.298753 |                    3 |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:29,075\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:30,755\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:32,415\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:34 (running for 00:02:04.46)\n",
      "Memory usage on this node: 17.1/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.34378675544549264 | Iter 3.000: -0.5793413854557841\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00007 | RUNNING    | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 | 0.165347 |                    6 |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:34,108\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:35,791\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:37,474\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:39 (running for 00:02:09.52)\n",
      "Memory usage on this node: 17.1/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.34378675544549264 | Iter 3.000: -0.5793413854557841\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00007 | RUNNING    | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 | 0.170888 |                    9 |\n",
      "| train_model_25b11_00008 | PENDING    |                 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:39,170\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=15980)\u001b[0m 2023-01-30 19:08:40,863\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:46 (running for 00:02:17.13)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.34378675544549264 | Iter 3.000: -0.5793413854557841\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00008 | RUNNING    | 127.0.0.1:11172 |          128 |            3 |           32 | 0.121705   |          |                      |\n",
      "| train_model_25b11_00009 | PENDING    |                 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "| train_model_25b11_00007 | TERMINATED | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 | 0.162758 |                   10 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 3 | stacks        | ModuleList       | 19.8 M\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 19.8 M    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 19.8 M    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 158.640   Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 2023-01-30 19:08:47,766\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 2023-01-30 19:08:48,431\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 2023-01-30 19:08:49,078\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 2023-01-30 19:08:49,733\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 2023-01-30 19:08:50,385\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11172)\u001b[0m 2023-01-30 19:08:51,043\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:51 (running for 00:02:22.16)\n",
      "Memory usage on this node: 15.8/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5750834949737773\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00009 | RUNNING    | 127.0.0.1:29712 |          128 |            2 |          128 | 0.0280488  |          |                      |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "| train_model_25b11_00007 | TERMINATED | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 | 0.162758 |                   10 |\n",
      "| train_model_25b11_00008 | TERMINATED | 127.0.0.1:11172 |          128 |            3 |           32 | 0.121705   | 0.52124  |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 3 | stacks        | ModuleList       | 52.9 M\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 52.9 M    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 1.4 K     Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 52.9 M    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 423.039   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:08:59 (running for 00:02:29.88)\n",
      "Memory usage on this node: 17.9/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5750834949737773\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00009 | RUNNING    | 127.0.0.1:29712 |          128 |            2 |          128 | 0.0280488  | 6.74849  |                    1 |\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "| train_model_25b11_00007 | TERMINATED | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 | 0.162758 |                   10 |\n",
      "| train_model_25b11_00008 | TERMINATED | 127.0.0.1:11172 |          128 |            3 |           32 | 0.121705   | 0.52124  |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 2023-01-30 19:08:59,498\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 2023-01-30 19:09:01,299\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=29712)\u001b[0m 2023-01-30 19:09:03,135\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "2023-01-30 19:09:03,304\tINFO tune.py:762 -- Total run time: 153.65 seconds (153.50 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 19:09:03 (running for 00:02:33.52)\n",
      "Memory usage on this node: 17.9/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -0.14830844367209076 | Iter 6.000: -0.3542216255122099 | Iter 3.000: -0.5793413854557841\n",
      "Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/10.11 GiB heap, 0.0/5.06 GiB objects\n",
      "Current best trial: 25b11_00000 with MAPE=0.13582769678058468 and parameters={'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   num_blocks |   num_stacks |    dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------|\n",
      "| train_model_25b11_00000 | TERMINATED | 127.0.0.1:29528 |          128 |            3 |           32 | 0.021416   | 0.135828 |                   19 |\n",
      "| train_model_25b11_00001 | TERMINATED | 127.0.0.1:24532 |           16 |            4 |          128 | 0.0723749  | 1.32436  |                    3 |\n",
      "| train_model_25b11_00002 | TERMINATED | 127.0.0.1:5548  |          128 |            2 |           32 | 0.199994   | 0.731292 |                    6 |\n",
      "| train_model_25b11_00003 | TERMINATED | 127.0.0.1:1812  |           32 |            1 |           32 | 0.0429654  | 0.148242 |                   16 |\n",
      "| train_model_25b11_00004 | TERMINATED | 127.0.0.1:30116 |           64 |            2 |           32 | 0.145425   | 0.396265 |                    6 |\n",
      "| train_model_25b11_00005 | TERMINATED | 127.0.0.1:8832  |           16 |            1 |           32 | 0.142135   | 0.141592 |                   14 |\n",
      "| train_model_25b11_00006 | TERMINATED | 127.0.0.1:6084  |          128 |            1 |           64 | 0.0992416  | 0.749197 |                    3 |\n",
      "| train_model_25b11_00007 | TERMINATED | 127.0.0.1:15980 |           32 |            1 |          128 | 0.00862542 | 0.162758 |                   10 |\n",
      "| train_model_25b11_00008 | TERMINATED | 127.0.0.1:11172 |          128 |            3 |           32 | 0.121705   | 0.52124  |                    6 |\n",
      "| train_model_25b11_00009 | TERMINATED | 127.0.0.1:29712 |          128 |            2 |          128 | 0.0280488  | 0.9174   |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+--------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Best hyperparameters found were:  {'batch_size': 128, 'num_blocks': 3, 'num_stacks': 32, 'dropout': 0.021416047999032496}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torchmetrics import MeanAbsoluteError, MeanAbsolutePercentageError, MetricCollection\n",
    "\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.models import NBEATSModel\n",
    "\n",
    "def train_model(model_args, callbacks, train, val):\n",
    "    torch_metrics = MetricCollection([MeanAbsolutePercentageError(), MeanAbsoluteError()])\n",
    "    # Create the model using model_args from Ray Tune\n",
    "    model = NBEATSModel(\n",
    "        input_chunk_length=24,\n",
    "        output_chunk_length=12,\n",
    "        n_epochs=500,\n",
    "        torch_metrics=torch_metrics,\n",
    "        pl_trainer_kwargs={\"callbacks\": callbacks, \"enable_progress_bar\": False},\n",
    "        **model_args)\n",
    "\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "    )\n",
    "\n",
    "# Read data:\n",
    "series = AirPassengersDataset().load()\n",
    "\n",
    "# Create training and validation sets:\n",
    "train, val = series.split_after(pd.Timestamp(year=1957, month=12, day=1))\n",
    "\n",
    "# Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
    "transformer = Scaler()\n",
    "transformer.fit(train)\n",
    "train = transformer.transform(train)\n",
    "val = transformer.transform(val)\n",
    "\n",
    "# Early stop callback\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_MeanAbsolutePercentageError\",\n",
    "    patience=5,\n",
    "    min_delta=0.05,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# set up ray tune callback\n",
    "tune_callback = TuneReportCallback(\n",
    "    {\n",
    "        \"loss\": \"val_Loss\",\n",
    "        \"MAPE\": \"val_MeanAbsolutePercentageError\",\n",
    "    },\n",
    "    on=\"validation_end\",\n",
    ")\n",
    "\n",
    "# define the hyperparameter space\n",
    "config = {\n",
    "    \"batch_size\": tune.choice([16, 32, 64, 128]),\n",
    "    \"num_blocks\": tune.choice([1, 2, 3, 4, 5]),\n",
    "    \"num_stacks\": tune.choice([32, 64, 128]),\n",
    "    \"dropout\": tune.uniform(0, 0.2),\n",
    "}\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=list(config.keys()),\n",
    "    metric_columns=[\"loss\", \"MAPE\", \"training_iteration\"],\n",
    ")\n",
    "\n",
    "resources_per_trial = {\"cpu\": 16, \"gpu\": 1}\n",
    "\n",
    "# the number of combinations to try\n",
    "num_samples = 10\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=1000, grace_period=3, reduction_factor=2)\n",
    "\n",
    "train_fn_with_parameters = tune.with_parameters(\n",
    "    train_model, callbacks=[my_stopper, tune_callback], train=train, val=val,\n",
    ")\n",
    "\n",
    "# optimize hyperparameters by minimizing the MAPE on the validation set\n",
    "analysis = tune.run(\n",
    "    train_fn_with_parameters,\n",
    "    resources_per_trial=resources_per_trial,\n",
    "    # Using a metric instead of loss allows for\n",
    "    # comparison between different likelihood or loss functions.\n",
    "    metric=\"MAPE\",  # any value in TuneReportCallback.\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    name=\"tune_darts\",\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (default, Mar  2 2020, 13:06:26) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e4eb80f0a2947b2b4d080aa13c6d15fa0bc013de0be3eb24296c75a14756372"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
