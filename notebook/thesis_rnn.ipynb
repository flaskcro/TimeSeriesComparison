{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from datetime import datetime\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape, r2_score, rmse\n",
    "\n",
    "from darts.models import (\n",
    "    AutoARIMA,\n",
    "    Prophet,\n",
    "    RNNModel,\n",
    "    NBEATSModel,\n",
    "    BlockRNNModel,\n",
    "    TFTModel,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/rossmann-store-sales/train.csv', parse_dates=['Date'])\n",
    "df.sample(10)\n",
    "df = pd.concat([df.drop(columns='StateHoliday'), pd.get_dummies(df.StateHoliday, prefix='Holiday')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.read_csv('../input/rossmann-store-sales/store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(num):\n",
    "    sample = df[df.Store == num]\n",
    "    series = TimeSeries.from_dataframe(sample, 'Date', 'Sales')\n",
    "\n",
    "    train, test = series.split_before(pd.Timestamp(\"20150601\"))\n",
    "\n",
    "    transformer = Scaler()\n",
    "    train_transformed = transformer.fit_transform(train)\n",
    "    test_transformed = transformer.transform(test)\n",
    "\n",
    "    series_customers = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Customers')\n",
    "    series_open = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Open')\n",
    "    series_promo = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Promo')\n",
    "    series_school = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='SchoolHoliday')\n",
    "    series_weekday = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='DayOfWeek')\n",
    "    series_holiday_a = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Holiday_a')\n",
    "    series_holiday_b = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Holiday_b')\n",
    "    series_holiday_c = TimeSeries.from_dataframe(sample, time_col='Date', value_cols='Holiday_c')\n",
    "\n",
    "    customers_transformed = transformer.transform(series_customers)\n",
    "\n",
    "    covariates = series_customers.stack(series_open)\n",
    "    covariates = covariates.stack(series_promo)\n",
    "    covariates = covariates.stack(series_school)\n",
    "    covariates = covariates.stack(series_weekday)\n",
    "    covariates = covariates.stack(series_holiday_a)\n",
    "    covariates = covariates.stack(series_holiday_b)\n",
    "    covariates = covariates.stack(series_holiday_c)\n",
    "\n",
    "    train_covariates, test_covariates = covariates.split_before(pd.Timestamp(\"20150601\"))\n",
    "\n",
    "    return train_transformed, test_transformed, covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, covariates = get_sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df.groupby('Store').count().reset_index()[['Store', 'Date']]\n",
    "store_merged = pd.merge(store, cdf, left_on='Store', right_on='Store')\n",
    "store_merged = store_merged[store_merged.Date == 942]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(actual, pred):\n",
    "    return np.sqrt(np.mean( ((actual - pred) / actual)**2)) \n",
    "\n",
    "def smape(actual, pred):\n",
    "    return np.mean(np.abs(pred - actual) / ((np.abs(actual) + np.abs(pred))/2)) * 100\n",
    "\n",
    "def evaluate_model(model, train, test):\n",
    "    model.fit(train)\n",
    "    pred = model.predict(len(test))\n",
    "    test = test.pd_dataframe()\n",
    "    pred = pred.pd_dataframe()\n",
    "\n",
    "    test.columns = ['Actual']\n",
    "    pred.columns = ['Pred']\n",
    "\n",
    "    df = pd.concat([test, pred], axis=1)\n",
    "    df = df[df.Actual > 0]\n",
    "    return smape(df.Actual.values, df.Pred.values), rmspe(df.Actual.values, df.Pred.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed, test_transformed, covariates = get_sample(10)\n",
    "#nn_model = RNNModel(input_chunk_length=30, output_chunk_length=10)\n",
    "#rmspe_score, smape_score = evaluate_model(nn_model, train_transformed, test_transformed )\n",
    "#rmspe_score, smape_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_num_list = pd.read_csv('../input/store_list.csv').Store.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Naive LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    min_delta=0.05,\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_list = []\n",
    "rmspe_list = []\n",
    "\n",
    "from darts.metrics.metrics import mape\n",
    "for num in tqdm(store_num_list):\n",
    "    train_transformed, test_transformed, covariates = get_sample(num)\n",
    "    rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=30,\n",
    "            dropout=0.3,\n",
    "            batch_size=16,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "\n",
    "    smape_score, rmspe_score = evaluate_model(rnn_model, train_transformed, test_transformed )\n",
    "    rmspe_list.append(rmspe_score)\n",
    "    smape_list.append(smape_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_covariates, test_covariates = covariates.split_before(pd.Timestamp(\"20150601\"))\n",
    "\n",
    "rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=30,\n",
    "            dropout=0.3,\n",
    "            batch_size=16,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "rnn_model.fit(train_transformed)\n",
    "pred_demand_covs = rnn_model.predict(len(test_transformed))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "test_transformed.plot(label=\"actual\")\n",
    "pred_demand_covs.plot(label=\"forecast\")\n",
    "plt.title('RNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_covariates, test_covariates = covariates.split_before(pd.Timestamp(\"20150601\"))\n",
    "\n",
    "rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=30,\n",
    "            dropout=0.3,\n",
    "            batch_size=16,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "rnn_model.fit(train_transformed, future_covariates=covariates)\n",
    "pred_demand_covs = rnn_model.predict(len(test_transformed), future_covariates=covariates)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "test_transformed.plot(label=\"actual\")\n",
    "pred_demand_covs.plot(label=\"forecast\")\n",
    "plt.title('RNN with Covariates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(smape_list), np.std(smape_list), np.max(smape_list), np.min(smape_list))\n",
    "plt.hist(smape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(rmspe_list), np.std(rmspe_list), np.max(rmspe_list), np.min(rmspe_list))\n",
    "plt.hist(rmspe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = pd.DataFrame(\n",
    "    {\n",
    "        'rmspe' : rmspe_list,\n",
    "        'smape' : smape_list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Covrariate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_covariates_model(model, train, test, covariates):\n",
    "    model.fit(\n",
    "        train,\n",
    "        future_covariates=covariates,\n",
    "        verbose=False,\n",
    "    )\n",
    "    pred = model.predict(len(test), future_covariates=covariates)\n",
    "    pred = pred.pd_dataframe()\n",
    "    test = test.pd_dataframe()\n",
    "\n",
    "    test.columns = ['Actual']\n",
    "    pred.columns = ['Pred']\n",
    "\n",
    "    df = pd.concat([test, pred], axis=1)\n",
    "    df = df[df.Actual > 0]\n",
    "    return smape(df.Actual.values, df.Pred.values), rmspe(df.Actual.values, df.Pred.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_list = []\n",
    "rmspe_list = []\n",
    "\n",
    "for num in tqdm(store_num_list):\n",
    "    train_transformed, test_transformed, covariates = get_sample(num)\n",
    "    rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=30,\n",
    "            dropout=0.3,\n",
    "            batch_size=16,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "    mape_score, rmse_score = evaluate_covariates_model(rnn_model, train_transformed, test_transformed, covariates )\n",
    "    smape_list.append(mape_score)\n",
    "    rmspe_list.append(rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(smape_list), np.std(smape_list), np.max(smape_list), np.min(smape_list))\n",
    "plt.hist(smape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(rmspe_list), np.std(rmspe_list), np.max(rmspe_list), np.min(rmspe_list))\n",
    "plt.hist(rmspe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1['Store'] = store_num_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Probabilistic LSTM - DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.utils.likelihood_models import GaussianLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_list = []\n",
    "rmspe_list = []\n",
    "\n",
    "from darts.metrics.metrics import mape\n",
    "for num in tqdm(store_num_list):\n",
    "\n",
    "    train_transformed, test_transformed, covariates = get_sample(num)\n",
    "    rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=30,\n",
    "            dropout=0.3,\n",
    "            batch_size=16,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            likelihood=GaussianLikelihood(),\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "\n",
    "    mape_score, rmse_score = evaluate_covariates_model(rnn_model, train_transformed, test_transformed, covariates )\n",
    "    smape_list.append(mape_score)\n",
    "    rmspe_list.append(rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(smape_list), np.std(smape_list), np.max(smape_list), np.min(smape_list))\n",
    "plt.hist(smape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(rmspe_list), np.std(rmspe_list), np.max(rmspe_list), np.min(rmspe_list))\n",
    "plt.hist(rmspe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = pd.DataFrame(\n",
    "    {\n",
    "        'rmspe' : rmspe_list,\n",
    "        'smape' : smape_list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_covariates, test_covariates = covariates.split_before(pd.Timestamp(\"20150601\"))\n",
    "\n",
    "rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=30,\n",
    "            dropout=0.3,\n",
    "            batch_size=16,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            likelihood=GaussianLikelihood(),\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "rnn_model.fit(train_transformed, future_covariates=covariates)\n",
    "pred_demand_covs = rnn_model.predict(len(test_transformed), future_covariates=covariates)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "test_transformed.plot(label=\"actual\")\n",
    "pred_demand_covs.plot(label=\"forecast\")\n",
    "plt.title('DeepAR')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Result Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1['Store'] = store_num_list\n",
    "res2['Store'] = store_num_list\n",
    "res3['Store'] = store_num_list\n",
    "\n",
    "\n",
    "result = pd.merge(res1, res2, left_on='Store', right_on='Store')\n",
    "result = pd.merge(result, res3, left_on='Store', right_on='Store')\n",
    "result = result.set_index('Store')\n",
    "result = pd.merge(result, store[['Store','StoreType']], left_on='Store', right_on='Store')\n",
    "result.to_csv('../output/rnn_result.csv')\n",
    "result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torchmetrics import MeanAbsoluteError, MeanAbsolutePercentageError, MetricCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_args, callbacks, train, val, covariates):\n",
    "    torch_metrics = MetricCollection([MeanAbsolutePercentageError(), MeanAbsoluteError()])\n",
    "    # Create the model using model_args from Ray Tune\n",
    "    model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            torch_metrics=torch_metrics,\n",
    "            pl_trainer_kwargs = {\"callbacks\": callbacks, \"enable_progress_bar\": False},\n",
    "        **model_args)\n",
    "\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        future_covariates=covariates,\n",
    "        val_future_covariates=covariates\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, covariates = get_sample(10)\n",
    "train, val = train.split_after(pd.Timestamp(year=2015, month=4, day=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:00:08 (running for 00:00:00.20)\n",
      "Memory usage on this node: 15.9/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+\n",
      "| Trial name              | status   | loc             |   batch_size |   hidden_dim |   dropout |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+-----------|\n",
      "| train_model_c9a62_00000 | RUNNING  | 127.0.0.1:12020 |           16 |           60 | 0.251974  |\n",
      "| train_model_c9a62_00001 | PENDING  |                 |           16 |           60 | 0.255905  |\n",
      "| train_model_c9a62_00002 | PENDING  |                 |           32 |           30 | 0.265544  |\n",
      "| train_model_c9a62_00003 | PENDING  |                 |          128 |           90 | 0.0919255 |\n",
      "| train_model_c9a62_00004 | PENDING  |                 |           64 |           30 | 0.22092   |\n",
      "| train_model_c9a62_00005 | PENDING  |                 |           16 |           60 | 0.29673   |\n",
      "| train_model_c9a62_00006 | PENDING  |                 |          128 |           90 | 0.203408  |\n",
      "| train_model_c9a62_00007 | PENDING  |                 |           32 |           90 | 0.255402  |\n",
      "| train_model_c9a62_00008 | PENDING  |                 |           32 |           60 | 0.0061099 |\n",
      "| train_model_c9a62_00009 | PENDING  |                 |          128 |           60 | 0.166572  |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.251973683175626 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 3 | rnn           | LSTM             | 15.1 K\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 4 | V             | Linear           | 61    \n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 15.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 15.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 0.121     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">   MAPE</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag                               </th><th>hostname   </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_c9a62_00000</td><td style=\"text-align: right;\">83773.1</td><td>2023-01-30_23-00-17</td><td>True  </td><td>                </td><td>e36d4d461b5f4105bb515f48e7e5e59e</td><td>0_batch_size=16,dropout=0.2520,hidden_dim=60 </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">12020</td><td style=\"text-align: right;\">            3.27175 </td><td style=\"text-align: right;\">          0.504902</td><td style=\"text-align: right;\">      3.27175 </td><td style=\"text-align: right;\"> 1675090817</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>c9a62_00000</td><td style=\"text-align: right;\">   0.00403166</td></tr>\n",
       "<tr><td>train_model_c9a62_00001</td><td style=\"text-align: right;\">83773.1</td><td>2023-01-30_23-00-25</td><td>True  </td><td>                </td><td>a8d8b8b2215f4ccb9ad7604ae969dc0a</td><td>1_batch_size=16,dropout=0.2559,hidden_dim=60 </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">30732</td><td style=\"text-align: right;\">            3.08138 </td><td style=\"text-align: right;\">          0.477228</td><td style=\"text-align: right;\">      3.08138 </td><td style=\"text-align: right;\"> 1675090825</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>c9a62_00001</td><td style=\"text-align: right;\">   0.00506687</td></tr>\n",
       "<tr><td>train_model_c9a62_00002</td><td style=\"text-align: right;\">84218.2</td><td>2023-01-30_23-00-32</td><td>True  </td><td>                </td><td>beabb25f706944baaf66dbe243a7715e</td><td>                                             </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 9872</td><td style=\"text-align: right;\">            1.11676 </td><td style=\"text-align: right;\">          0.326571</td><td style=\"text-align: right;\">      1.11676 </td><td style=\"text-align: right;\"> 1675090832</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>c9a62_00002</td><td style=\"text-align: right;\">   0.0048008 </td></tr>\n",
       "<tr><td>train_model_c9a62_00003</td><td style=\"text-align: right;\">90838.5</td><td>2023-01-30_23-00-40</td><td>True  </td><td>                </td><td>50d45a7dca6a484fbdadb09554b4c136</td><td>                                             </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">30480</td><td style=\"text-align: right;\">            1.488   </td><td style=\"text-align: right;\">          0.43326 </td><td style=\"text-align: right;\">      1.488   </td><td style=\"text-align: right;\"> 1675090840</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>c9a62_00003</td><td style=\"text-align: right;\">   0.0049181 </td></tr>\n",
       "<tr><td>train_model_c9a62_00004</td><td style=\"text-align: right;\">89244.7</td><td>2023-01-30_23-00-46</td><td>True  </td><td>                </td><td>f4b1046f949849aa8d6993695e28a697</td><td>                                             </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">17452</td><td style=\"text-align: right;\">            0.998566</td><td style=\"text-align: right;\">          0.289863</td><td style=\"text-align: right;\">      0.998566</td><td style=\"text-align: right;\"> 1675090846</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>c9a62_00004</td><td style=\"text-align: right;\">   0.00519109</td></tr>\n",
       "<tr><td>train_model_c9a62_00005</td><td style=\"text-align: right;\">83773.1</td><td>2023-01-30_23-00-55</td><td>True  </td><td>                </td><td>388ee560d0674b7ca5a8c961923a066d</td><td>5_batch_size=16,dropout=0.2967,hidden_dim=60 </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 6960</td><td style=\"text-align: right;\">            3.09162 </td><td style=\"text-align: right;\">          0.483547</td><td style=\"text-align: right;\">      3.09162 </td><td style=\"text-align: right;\"> 1675090855</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>c9a62_00005</td><td style=\"text-align: right;\">   0.00353503</td></tr>\n",
       "<tr><td>train_model_c9a62_00006</td><td style=\"text-align: right;\">90838.5</td><td>2023-01-30_23-01-03</td><td>True  </td><td>                </td><td>d6cfdc9f5e3d4d44a1f867e389e02a9f</td><td>                                             </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">30700</td><td style=\"text-align: right;\">            1.47554 </td><td style=\"text-align: right;\">          0.428732</td><td style=\"text-align: right;\">      1.47554 </td><td style=\"text-align: right;\"> 1675090863</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>c9a62_00006</td><td style=\"text-align: right;\">   0.00416946</td></tr>\n",
       "<tr><td>train_model_c9a62_00007</td><td style=\"text-align: right;\">85308.6</td><td>2023-01-30_23-01-11</td><td>True  </td><td>                </td><td>bd486a797fc24f91a471d1d72f4f4fce</td><td>                                             </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">26168</td><td style=\"text-align: right;\">            3.25258 </td><td style=\"text-align: right;\">          0.52614 </td><td style=\"text-align: right;\">      3.25258 </td><td style=\"text-align: right;\"> 1675090871</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>c9a62_00007</td><td style=\"text-align: right;\">   0.00572491</td></tr>\n",
       "<tr><td>train_model_c9a62_00008</td><td style=\"text-align: right;\">83675.8</td><td>2023-01-30_23-01-20</td><td>True  </td><td>                </td><td>c1426a461245490b8e733ebbc4bdb3cf</td><td>8_batch_size=32,dropout=0.0061,hidden_dim=60 </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">32076</td><td style=\"text-align: right;\">            2.46181 </td><td style=\"text-align: right;\">          0.386256</td><td style=\"text-align: right;\">      2.46181 </td><td style=\"text-align: right;\"> 1675090880</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>c9a62_00008</td><td style=\"text-align: right;\">   0.00398135</td></tr>\n",
       "<tr><td>train_model_c9a62_00009</td><td style=\"text-align: right;\">71638.1</td><td>2023-01-30_23-01-27</td><td>True  </td><td>                </td><td>918bf9f049eb4d3bb6d591c1123fc121</td><td>9_batch_size=128,dropout=0.1666,hidden_dim=60</td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 9976</td><td style=\"text-align: right;\">            1.9888  </td><td style=\"text-align: right;\">          0.298176</td><td style=\"text-align: right;\">      1.9888  </td><td style=\"text-align: right;\"> 1675090887</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>c9a62_00009</td><td style=\"text-align: right;\">   0.00416851</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:00:14 (running for 00:00:05.91)\n",
      "Memory usage on this node: 16.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=76198.34725618408 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status   | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00000 | RUNNING  | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 76198.3 |                    1 |\n",
      "| train_model_c9a62_00001 | PENDING  |                 |           16 |           60 | 0.255905  |         |                      |\n",
      "| train_model_c9a62_00002 | PENDING  |                 |           32 |           30 | 0.265544  |         |                      |\n",
      "| train_model_c9a62_00003 | PENDING  |                 |          128 |           90 | 0.0919255 |         |                      |\n",
      "| train_model_c9a62_00004 | PENDING  |                 |           64 |           30 | 0.22092   |         |                      |\n",
      "| train_model_c9a62_00005 | PENDING  |                 |           16 |           60 | 0.29673   |         |                      |\n",
      "| train_model_c9a62_00006 | PENDING  |                 |          128 |           90 | 0.203408  |         |                      |\n",
      "| train_model_c9a62_00007 | PENDING  |                 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING  |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING  |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 2023-01-30 23:00:14,529\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 2023-01-30 23:00:15,064\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 2023-01-30 23:00:15,577\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 2023-01-30 23:00:16,110\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 2023-01-30 23:00:16,630\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=12020)\u001b[0m 2023-01-30 23:00:17,135\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:00:22 (running for 00:00:13.88)\n",
      "Memory usage on this node: 16.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -82886.30642788149\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00001 | RUNNING    | 127.0.0.1:30732 |           16 |           60 | 0.255905  |         |                      |\n",
      "| train_model_c9a62_00002 | PENDING    |                 |           32 |           30 | 0.265544  |         |                      |\n",
      "| train_model_c9a62_00003 | PENDING    |                 |          128 |           90 | 0.0919255 |         |                      |\n",
      "| train_model_c9a62_00004 | PENDING    |                 |           64 |           30 | 0.22092   |         |                      |\n",
      "| train_model_c9a62_00005 | PENDING    |                 |           16 |           60 | 0.29673   |         |                      |\n",
      "| train_model_c9a62_00006 | PENDING    |                 |          128 |           90 | 0.203408  |         |                      |\n",
      "| train_model_c9a62_00007 | PENDING    |                 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING    |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25590459031565943 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 3 | rnn           | LSTM             | 15.1 K\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 4 | V             | Linear           | 61    \n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 15.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 15.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 0.121     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 2023-01-30 23:00:23,150\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 2023-01-30 23:00:23,631\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 2023-01-30 23:00:24,113\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 2023-01-30 23:00:24,587\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 2023-01-30 23:00:25,073\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30732)\u001b[0m 2023-01-30 23:00:25,551\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:00:31 (running for 00:00:22.88)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -82886.30642788149\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00002 | RUNNING    | 127.0.0.1:9872  |           32 |           30 | 0.265544  |         |                      |\n",
      "| train_model_c9a62_00003 | PENDING    |                 |          128 |           90 | 0.0919255 |         |                      |\n",
      "| train_model_c9a62_00004 | PENDING    |                 |           64 |           30 | 0.22092   |         |                      |\n",
      "| train_model_c9a62_00005 | PENDING    |                 |           16 |           60 | 0.29673   |         |                      |\n",
      "| train_model_c9a62_00006 | PENDING    |                 |          128 |           90 | 0.203408  |         |                      |\n",
      "| train_model_c9a62_00007 | PENDING    |                 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING    |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.26554406829569244 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 3 | rnn           | LSTM             | 4.0 K \n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 4 | V             | Linear           | 31    \n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 4.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 4.0 K     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 0.032     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 2023-01-30 23:00:31,937\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 2023-01-30 23:00:32,285\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=9872)\u001b[0m 2023-01-30 23:00:32,611\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:00:38 (running for 00:00:29.95)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -82886.30642788149\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00003 | RUNNING    | 127.0.0.1:30480 |          128 |           90 | 0.0919255 |         |                      |\n",
      "| train_model_c9a62_00004 | PENDING    |                 |           64 |           30 | 0.22092   |         |                      |\n",
      "| train_model_c9a62_00005 | PENDING    |                 |           16 |           60 | 0.29673   |         |                      |\n",
      "| train_model_c9a62_00006 | PENDING    |                 |          128 |           90 | 0.203408  |         |                      |\n",
      "| train_model_c9a62_00007 | PENDING    |                 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING    |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.0919254982216968 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 3 | rnn           | LSTM             | 33.5 K\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 4 | V             | Linear           | 91    \n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 33.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 33.6 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 0.269     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 2023-01-30 23:00:39,232\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 2023-01-30 23:00:39,676\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30480)\u001b[0m 2023-01-30 23:00:40,109\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:00:45 (running for 00:00:37.00)\n",
      "Memory usage on this node: 16.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -83552.24979325336\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00004 | RUNNING    | 127.0.0.1:17452 |           64 |           30 | 0.22092   |         |                      |\n",
      "| train_model_c9a62_00005 | PENDING    |                 |           16 |           60 | 0.29673   |         |                      |\n",
      "| train_model_c9a62_00006 | PENDING    |                 |          128 |           90 | 0.203408  |         |                      |\n",
      "| train_model_c9a62_00007 | PENDING    |                 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING    |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "| train_model_c9a62_00003 | TERMINATED | 127.0.0.1:30480 |          128 |           90 | 0.0919255 | 90838.5 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.22091952609016197 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 3 | rnn           | LSTM             | 4.0 K \n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 4 | V             | Linear           | 31    \n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 4.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 4.0 K     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 0.032     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 2023-01-30 23:00:46,195\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 2023-01-30 23:00:46,486\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=17452)\u001b[0m 2023-01-30 23:00:46,775\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:00:52 (running for 00:00:43.99)\n",
      "Memory usage on this node: 16.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -84218.19315862523\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00005 | RUNNING    | 127.0.0.1:6960  |           16 |           60 | 0.29673   |         |                      |\n",
      "| train_model_c9a62_00006 | PENDING    |                 |          128 |           90 | 0.203408  |         |                      |\n",
      "| train_model_c9a62_00007 | PENDING    |                 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING    |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "| train_model_c9a62_00003 | TERMINATED | 127.0.0.1:30480 |          128 |           90 | 0.0919255 | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00004 | TERMINATED | 127.0.0.1:17452 |           64 |           30 | 0.22092   | 89244.7 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.29673034789542696 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 3 | rnn           | LSTM             | 15.1 K\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 4 | V             | Linear           | 61    \n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 15.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 15.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 0.121     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 2023-01-30 23:00:53,452\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 2023-01-30 23:00:53,939\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 2023-01-30 23:00:54,407\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 2023-01-30 23:00:54,922\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 2023-01-30 23:00:55,414\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=6960)\u001b[0m 2023-01-30 23:00:55,897\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:01:01 (running for 00:00:53.05)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -83552.24979325336\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00006 | RUNNING    | 127.0.0.1:30700 |          128 |           90 | 0.203408  |         |                      |\n",
      "| train_model_c9a62_00007 | PENDING    |                 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING    |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "| train_model_c9a62_00003 | TERMINATED | 127.0.0.1:30480 |          128 |           90 | 0.0919255 | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00004 | TERMINATED | 127.0.0.1:17452 |           64 |           30 | 0.22092   | 89244.7 |                    3 |\n",
      "| train_model_c9a62_00005 | TERMINATED | 127.0.0.1:6960  |           16 |           60 | 0.29673   | 83773.1 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.20340830383208086 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 3 | rnn           | LSTM             | 33.5 K\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 4 | V             | Linear           | 91    \n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 33.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 33.6 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 0.269     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 2023-01-30 23:01:02,281\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 2023-01-30 23:01:02,715\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=30700)\u001b[0m 2023-01-30 23:01:03,144\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:01:08 (running for 00:01:00.13)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -84218.19315862523\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00007 | RUNNING    | 127.0.0.1:26168 |           32 |           90 | 0.255402  |         |                      |\n",
      "| train_model_c9a62_00008 | PENDING    |                 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "| train_model_c9a62_00003 | TERMINATED | 127.0.0.1:30480 |          128 |           90 | 0.0919255 | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00004 | TERMINATED | 127.0.0.1:17452 |           64 |           30 | 0.22092   | 89244.7 |                    3 |\n",
      "| train_model_c9a62_00005 | TERMINATED | 127.0.0.1:6960  |           16 |           60 | 0.29673   | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00006 | TERMINATED | 127.0.0.1:30700 |          128 |           90 | 0.203408  | 90838.5 |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25540233871813645 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 3 | rnn           | LSTM             | 33.5 K\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 4 | V             | Linear           | 91    \n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 33.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 33.6 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 0.269     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 2023-01-30 23:01:09,380\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 2023-01-30 23:01:09,892\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 2023-01-30 23:01:10,399\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 2023-01-30 23:01:10,909\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 2023-01-30 23:01:11,424\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=26168)\u001b[0m 2023-01-30 23:01:11,950\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:01:17 (running for 00:01:09.15)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -84048.08211406544\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00000 with MAPE=83773.08526340287 and parameters={'batch_size': 16, 'hidden_dim': 60, 'dropout': 0.251973683175626}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00008 | RUNNING    | 127.0.0.1:32076 |           32 |           60 | 0.0061099 |         |                      |\n",
      "| train_model_c9a62_00009 | PENDING    |                 |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "| train_model_c9a62_00003 | TERMINATED | 127.0.0.1:30480 |          128 |           90 | 0.0919255 | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00004 | TERMINATED | 127.0.0.1:17452 |           64 |           30 | 0.22092   | 89244.7 |                    3 |\n",
      "| train_model_c9a62_00005 | TERMINATED | 127.0.0.1:6960  |           16 |           60 | 0.29673   | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00006 | TERMINATED | 127.0.0.1:30700 |          128 |           90 | 0.203408  | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00007 | TERMINATED | 127.0.0.1:26168 |           32 |           90 | 0.255402  | 85308.6 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.006109897189298941 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 3 | rnn           | LSTM             | 15.1 K\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 4 | V             | Linear           | 61    \n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 15.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 15.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 0.121     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 2023-01-30 23:01:18,358\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 2023-01-30 23:01:18,759\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 2023-01-30 23:01:19,140\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 2023-01-30 23:01:19,518\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 2023-01-30 23:01:19,888\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32076)\u001b[0m 2023-01-30 23:01:20,274\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:01:25 (running for 00:01:17.19)\n",
      "Memory usage on this node: 16.2/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -83877.97106950564\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00008 with MAPE=83675.84334219756 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.006109897189298941}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00009 | RUNNING    | 127.0.0.1:9976  |          128 |           60 | 0.166572  |         |                      |\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "| train_model_c9a62_00003 | TERMINATED | 127.0.0.1:30480 |          128 |           90 | 0.0919255 | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00004 | TERMINATED | 127.0.0.1:17452 |           64 |           30 | 0.22092   | 89244.7 |                    3 |\n",
      "| train_model_c9a62_00005 | TERMINATED | 127.0.0.1:6960  |           16 |           60 | 0.29673   | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00006 | TERMINATED | 127.0.0.1:30700 |          128 |           90 | 0.203408  | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00007 | TERMINATED | 127.0.0.1:26168 |           32 |           90 | 0.255402  | 85308.6 |                    6 |\n",
      "| train_model_c9a62_00008 | TERMINATED | 127.0.0.1:32076 |           32 |           60 | 0.0061099 | 83675.8 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16657176761791098 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 3 | rnn           | LSTM             | 15.1 K\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 4 | V             | Linear           | 61    \n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 15.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 15.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 0.121     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 2023-01-30 23:01:26,321\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 2023-01-30 23:01:26,623\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 2023-01-30 23:01:26,908\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 2023-01-30 23:01:27,210\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 2023-01-30 23:01:27,514\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=9976)\u001b[0m 2023-01-30 23:01:27,813\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "2023-01-30 23:01:27,992\tINFO tune.py:762 -- Total run time: 79.37 seconds (79.22 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:01:27 (running for 00:01:19.25)\n",
      "Memory usage on this node: 16.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: -83773.08526340287 | Iter 3.000: -83382.13874869357\n",
      "Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: c9a62_00009 with MAPE=71638.06035203935 and parameters={'batch_size': 128, 'hidden_dim': 60, 'dropout': 0.16657176761791098}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_c9a62_00000 | TERMINATED | 127.0.0.1:12020 |           16 |           60 | 0.251974  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00001 | TERMINATED | 127.0.0.1:30732 |           16 |           60 | 0.255905  | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00002 | TERMINATED | 127.0.0.1:9872  |           32 |           30 | 0.265544  | 84218.2 |                    3 |\n",
      "| train_model_c9a62_00003 | TERMINATED | 127.0.0.1:30480 |          128 |           90 | 0.0919255 | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00004 | TERMINATED | 127.0.0.1:17452 |           64 |           30 | 0.22092   | 89244.7 |                    3 |\n",
      "| train_model_c9a62_00005 | TERMINATED | 127.0.0.1:6960  |           16 |           60 | 0.29673   | 83773.1 |                    6 |\n",
      "| train_model_c9a62_00006 | TERMINATED | 127.0.0.1:30700 |          128 |           90 | 0.203408  | 90838.5 |                    3 |\n",
      "| train_model_c9a62_00007 | TERMINATED | 127.0.0.1:26168 |           32 |           90 | 0.255402  | 85308.6 |                    6 |\n",
      "| train_model_c9a62_00008 | TERMINATED | 127.0.0.1:32076 |           32 |           60 | 0.0061099 | 83675.8 |                    6 |\n",
      "| train_model_c9a62_00009 | TERMINATED | 127.0.0.1:9976  |          128 |           60 | 0.166572  | 71638.1 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n",
      "Best hyperparameters found were:  {'batch_size': 128, 'hidden_dim': 60, 'dropout': 0.16657176761791098}\n"
     ]
    }
   ],
   "source": [
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_MeanAbsolutePercentageError\",\n",
    "    patience=5,\n",
    "    min_delta=0.05,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# set up ray tune callback\n",
    "tune_callback = TuneReportCallback(\n",
    "    {\n",
    "        \"loss\": \"val_Loss\",\n",
    "        \"MAPE\": \"val_MeanAbsolutePercentageError\",\n",
    "    },\n",
    "    on=\"validation_end\",\n",
    ")\n",
    "\n",
    "# define the hyperparameter space\n",
    "config = {\n",
    "    \"batch_size\": tune.choice([16, 32, 64, 128]),\n",
    "    \"hidden_dim\": tune.choice([30, 60, 90]),\n",
    "    \"dropout\": tune.uniform(0, 0.3),\n",
    "}\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=list(config.keys()),\n",
    "    metric_columns=[\"loss\", \"MAPE\", \"training_iteration\"],\n",
    ")\n",
    "\n",
    "resources_per_trial = {\"cpu\": 16, \"gpu\": 1}\n",
    "\n",
    "# the number of combinations to try\n",
    "num_samples = 10\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=1000, grace_period=3, reduction_factor=2)\n",
    "\n",
    "train_fn_with_parameters = tune.with_parameters(\n",
    "    train_model, callbacks=[my_stopper, tune_callback], train=train, val=val, covariates=covariates\n",
    ")\n",
    "\n",
    "# optimize hyperparameters by minimizing the MAPE on the validation set\n",
    "analysis = tune.run(\n",
    "    train_fn_with_parameters,\n",
    "    resources_per_trial=resources_per_trial,\n",
    "    # Using a metric instead of loss allows for\n",
    "    # comparison between different likelihood or loss functions.\n",
    "    metric=\"MAPE\",  # any value in TuneReportCallback.\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    name=\"tune_darts\",\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  1%|          | 1/100 [00:25<42:51, 25.98s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  2%|         | 2/100 [00:49<39:50, 24.39s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  3%|         | 3/100 [01:12<38:26, 23.78s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  4%|         | 4/100 [01:35<37:35, 23.50s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  5%|         | 5/100 [01:58<36:52, 23.29s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  6%|         | 6/100 [02:21<36:36, 23.36s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  7%|         | 7/100 [02:47<37:16, 24.05s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  8%|         | 8/100 [03:13<38:10, 24.90s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  9%|         | 9/100 [03:37<37:19, 24.61s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 10%|         | 10/100 [04:01<36:24, 24.27s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 11%|         | 11/100 [04:24<35:29, 23.92s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 12%|        | 12/100 [04:47<34:39, 23.64s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 13%|        | 13/100 [05:10<33:58, 23.43s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 14%|        | 14/100 [05:36<34:30, 24.08s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 15%|        | 15/100 [06:05<36:18, 25.63s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 16%|        | 16/100 [06:29<35:18, 25.22s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 17%|        | 17/100 [06:53<34:10, 24.70s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 18%|        | 18/100 [07:16<33:11, 24.28s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 19%|        | 19/100 [07:39<32:28, 24.05s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 20%|        | 20/100 [08:13<35:42, 26.79s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 21%|        | 21/100 [09:06<45:37, 34.65s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 22%|       | 22/100 [09:29<40:28, 31.14s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 23%|       | 23/100 [09:55<38:04, 29.67s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 24%|       | 24/100 [10:18<35:13, 27.81s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 25%|       | 25/100 [10:43<33:30, 26.80s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 26%|       | 26/100 [11:09<32:41, 26.51s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 27%|       | 27/100 [11:35<32:21, 26.60s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 28%|       | 28/100 [11:59<30:56, 25.78s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 29%|       | 29/100 [12:22<29:33, 24.98s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 30%|       | 30/100 [12:46<28:47, 24.68s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 31%|       | 31/100 [13:10<27:58, 24.33s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 32%|      | 32/100 [13:34<27:36, 24.36s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 33%|      | 33/100 [13:58<26:57, 24.15s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 34%|      | 34/100 [14:22<26:26, 24.04s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 35%|      | 35/100 [14:45<25:55, 23.93s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 36%|      | 36/100 [15:09<25:30, 23.91s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 37%|      | 37/100 [15:33<24:59, 23.81s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 38%|      | 38/100 [15:57<24:41, 23.89s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 39%|      | 39/100 [16:21<24:20, 23.95s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 40%|      | 40/100 [16:46<24:22, 24.38s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 41%|      | 41/100 [17:10<23:50, 24.24s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 42%|     | 42/100 [17:34<23:25, 24.23s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 43%|     | 43/100 [17:58<22:57, 24.17s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 44%|     | 44/100 [18:23<22:39, 24.28s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 45%|     | 45/100 [18:48<22:35, 24.64s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 46%|     | 46/100 [19:13<22:05, 24.55s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 47%|     | 47/100 [19:36<21:23, 24.22s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 48%|     | 48/100 [20:00<20:49, 24.02s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 49%|     | 49/100 [20:26<20:58, 24.69s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 50%|     | 50/100 [20:51<20:31, 24.63s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 51%|     | 51/100 [21:14<19:42, 24.13s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 52%|    | 52/100 [21:38<19:19, 24.16s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 53%|    | 53/100 [22:02<19:03, 24.32s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 54%|    | 54/100 [22:26<18:23, 24.00s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 55%|    | 55/100 [22:50<18:05, 24.11s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 56%|    | 56/100 [23:17<18:12, 24.84s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 57%|    | 57/100 [23:40<17:25, 24.31s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 58%|    | 58/100 [24:02<16:36, 23.73s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 59%|    | 59/100 [24:25<15:59, 23.41s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 60%|    | 60/100 [24:47<15:28, 23.20s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 61%|    | 61/100 [25:10<14:59, 23.07s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 62%|   | 62/100 [25:33<14:32, 22.97s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 63%|   | 63/100 [25:56<14:06, 22.87s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 64%|   | 64/100 [26:18<13:41, 22.81s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 65%|   | 65/100 [26:41<13:18, 22.80s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 66%|   | 66/100 [27:04<12:52, 22.72s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 67%|   | 67/100 [27:26<12:29, 22.73s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 68%|   | 68/100 [27:49<12:06, 22.70s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 69%|   | 69/100 [28:12<11:42, 22.66s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 70%|   | 70/100 [28:35<11:22, 22.77s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 71%|   | 71/100 [29:01<11:30, 23.79s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 72%|  | 72/100 [29:25<11:08, 23.87s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 73%|  | 73/100 [29:48<10:42, 23.78s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 74%|  | 74/100 [30:13<10:23, 23.98s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 75%|  | 75/100 [30:40<10:24, 24.99s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 76%|  | 76/100 [31:04<09:49, 24.55s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 77%|  | 77/100 [31:27<09:14, 24.10s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 78%|  | 78/100 [31:50<08:47, 23.96s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 79%|  | 79/100 [32:14<08:18, 23.76s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 80%|  | 80/100 [32:37<07:54, 23.74s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 81%|  | 81/100 [33:01<07:28, 23.61s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 82%| | 82/100 [33:24<07:03, 23.53s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 83%| | 83/100 [33:47<06:37, 23.40s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 84%| | 84/100 [34:11<06:15, 23.48s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 85%| | 85/100 [34:34<05:50, 23.35s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 86%| | 86/100 [34:58<05:31, 23.69s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 87%| | 87/100 [35:23<05:13, 24.14s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 88%| | 88/100 [35:49<04:56, 24.68s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 89%| | 89/100 [36:15<04:34, 24.91s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 90%| | 90/100 [36:39<04:06, 24.60s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 91%| | 91/100 [37:02<03:38, 24.27s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 92%|| 92/100 [37:28<03:17, 24.69s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 93%|| 93/100 [37:55<02:57, 25.42s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 94%|| 94/100 [38:21<02:33, 25.55s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 95%|| 95/100 [38:46<02:07, 25.41s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 96%|| 96/100 [39:10<01:40, 25.12s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 97%|| 97/100 [39:36<01:15, 25.24s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 98%|| 98/100 [40:01<00:50, 25.11s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 99%|| 99/100 [40:24<00:24, 24.64s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "100%|| 100/100 [40:47<00:00, 24.48s/it]\n"
     ]
    }
   ],
   "source": [
    "smape_list = []\n",
    "rmspe_list = []\n",
    "\n",
    "for num in tqdm(store_num_list):\n",
    "    train_transformed, test_transformed, covariates = get_sample(num)\n",
    "    rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=60,\n",
    "            dropout= 0.16657176761791098,\n",
    "            batch_size=128,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "    mape_score, rmse_score = evaluate_covariates_model(rnn_model, train_transformed, test_transformed, covariates )\n",
    "    smape_list.append(mape_score)\n",
    "    rmspe_list.append(rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmspe</th>\n",
       "      <th>smape</th>\n",
       "      <th>Store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.086285</td>\n",
       "      <td>7.701973</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049630</td>\n",
       "      <td>3.829394</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052607</td>\n",
       "      <td>4.260640</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070980</td>\n",
       "      <td>5.526770</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060497</td>\n",
       "      <td>4.899070</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.050640</td>\n",
       "      <td>4.221876</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.079968</td>\n",
       "      <td>7.179742</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.050098</td>\n",
       "      <td>4.260537</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.065118</td>\n",
       "      <td>5.406682</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.049300</td>\n",
       "      <td>3.988700</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rmspe     smape  Store\n",
       "0   0.086285  7.701973     10\n",
       "1   0.049630  3.829394     62\n",
       "2   0.052607  4.260640    110\n",
       "3   0.070980  5.526770    163\n",
       "4   0.060497  4.899070    300\n",
       "..       ...       ...    ...\n",
       "95  0.050640  4.221876    960\n",
       "96  0.079968  7.179742   1010\n",
       "97  0.050098  4.260537   1046\n",
       "98  0.065118  5.406682   1089\n",
       "99  0.049300  3.988700   1101\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_h1 = pd.DataFrame(\n",
    "    {\n",
    "        'rmspe' : rmspe_list,\n",
    "        'smape' : smape_list,\n",
    "    }\n",
    ")\n",
    "res_h1['Store'] = store_num_list\n",
    "res_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmspe      0.076045\n",
       "smape      6.459042\n",
       "Store    585.390000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_h1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR - Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "def train_model(model_args, callbacks, train, val, covariates):\n",
    "    torch_metrics = MetricCollection([MeanAbsolutePercentageError(), MeanAbsoluteError()])\n",
    "    # Create the model using model_args from Ray Tune\n",
    "    model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            torch_metrics=torch_metrics,\n",
    "            likelihood=GaussianLikelihood(),\n",
    "            pl_trainer_kwargs = {\"callbacks\": callbacks, \"enable_progress_bar\": False},\n",
    "        **model_args)\n",
    "\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        future_covariates=covariates,\n",
    "        val_future_covariates=covariates\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, covariates = get_sample(10)\n",
    "train, val = train.split_after(pd.Timestamp(year=2015, month=4, day=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:48:57 (running for 00:00:00.19)\n",
      "Memory usage on this node: 17.9/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+\n",
      "| Trial name              | status   | loc             |   batch_size |   hidden_dim |   dropout |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+-----------|\n",
      "| train_model_9b6c5_00000 | RUNNING  | 127.0.0.1:11872 |           32 |           60 | 0.283982  |\n",
      "| train_model_9b6c5_00001 | PENDING  |                 |           16 |           60 | 0.0597332 |\n",
      "| train_model_9b6c5_00002 | PENDING  |                 |          128 |           90 | 0.218672  |\n",
      "| train_model_9b6c5_00003 | PENDING  |                 |           64 |           60 | 0.199901  |\n",
      "| train_model_9b6c5_00004 | PENDING  |                 |          128 |           60 | 0.0941246 |\n",
      "| train_model_9b6c5_00005 | PENDING  |                 |           16 |           60 | 0.0678516 |\n",
      "| train_model_9b6c5_00006 | PENDING  |                 |           16 |           90 | 0.295774  |\n",
      "| train_model_9b6c5_00007 | PENDING  |                 |          128 |           30 | 0.141403  |\n",
      "| train_model_9b6c5_00008 | PENDING  |                 |          128 |           30 | 0.147392  |\n",
      "| train_model_9b6c5_00009 | PENDING  |                 |           16 |           30 | 0.299093  |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.28398153787733454 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 3 | rnn           | LSTM             | 17.0 K\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 4 | V             | Linear           | 122   \n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 17.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 17.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 0.137     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">     MAPE</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag                              </th><th>hostname   </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_9b6c5_00000</td><td style=\"text-align: right;\"> 1246.77 </td><td>2023-01-30_23-49-13</td><td>True  </td><td>                </td><td>7576b71a1e234bbcbfe7702282b19374</td><td>0_batch_size=32,dropout=0.2840,hidden_dim=60</td><td>Tensortorch</td><td style=\"text-align: right;\">                        21</td><td>127.0.0.1</td><td style=\"text-align: right;\">11872</td><td style=\"text-align: right;\">             9.93027</td><td style=\"text-align: right;\">          0.458184</td><td style=\"text-align: right;\">       9.93027</td><td style=\"text-align: right;\"> 1675093753</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  21</td><td>9b6c5_00000</td><td style=\"text-align: right;\">   0.0029943 </td></tr>\n",
       "<tr><td>train_model_9b6c5_00001</td><td style=\"text-align: right;\"> 2422.18 </td><td>2023-01-30_23-49-22</td><td>True  </td><td>                </td><td>4d4e6d548daf4db7881c9d6a55242e64</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">32088</td><td style=\"text-align: right;\">             3.71122</td><td style=\"text-align: right;\">          0.57908 </td><td style=\"text-align: right;\">       3.71122</td><td style=\"text-align: right;\"> 1675093762</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>9b6c5_00001</td><td style=\"text-align: right;\">   0.00359297</td></tr>\n",
       "<tr><td>train_model_9b6c5_00002</td><td style=\"text-align: right;\">21652.3  </td><td>2023-01-30_23-49-29</td><td>True  </td><td>                </td><td>1e641dadc7cb41fb8a0cd01cf7c27909</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">34416</td><td style=\"text-align: right;\">             1.82825</td><td style=\"text-align: right;\">          0.544136</td><td style=\"text-align: right;\">       1.82825</td><td style=\"text-align: right;\"> 1675093769</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>9b6c5_00002</td><td style=\"text-align: right;\">   0.00513673</td></tr>\n",
       "<tr><td>train_model_9b6c5_00003</td><td style=\"text-align: right;\">16413.7  </td><td>2023-01-30_23-49-36</td><td>True  </td><td>                </td><td>61d892dd525845a3be3b253637508479</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">  476</td><td style=\"text-align: right;\">             1.39729</td><td style=\"text-align: right;\">          0.410841</td><td style=\"text-align: right;\">       1.39729</td><td style=\"text-align: right;\"> 1675093776</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>9b6c5_00003</td><td style=\"text-align: right;\">   0.00399899</td></tr>\n",
       "<tr><td>train_model_9b6c5_00004</td><td style=\"text-align: right;\">39777.1  </td><td>2023-01-30_23-49-43</td><td>True  </td><td>                </td><td>b23769bec92e402b9376e2dd1a391063</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">18048</td><td style=\"text-align: right;\">             1.34503</td><td style=\"text-align: right;\">          0.387234</td><td style=\"text-align: right;\">       1.34503</td><td style=\"text-align: right;\"> 1675093783</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>9b6c5_00004</td><td style=\"text-align: right;\">   0.00490594</td></tr>\n",
       "<tr><td>train_model_9b6c5_00005</td><td style=\"text-align: right;\"> 2422.18 </td><td>2023-01-30_23-49-53</td><td>True  </td><td>                </td><td>8b9f68fa284843cabaa1f1b995e39804</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\">27488</td><td style=\"text-align: right;\">             3.88326</td><td style=\"text-align: right;\">          0.600273</td><td style=\"text-align: right;\">       3.88326</td><td style=\"text-align: right;\"> 1675093793</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>9b6c5_00005</td><td style=\"text-align: right;\">   0.00497699</td></tr>\n",
       "<tr><td>train_model_9b6c5_00006</td><td style=\"text-align: right;\"> 3841.69 </td><td>2023-01-30_23-50-04</td><td>True  </td><td>                </td><td>6dfa104beeca403d991bcf861f53fcca</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         6</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 1548</td><td style=\"text-align: right;\">             4.90342</td><td style=\"text-align: right;\">          0.8762  </td><td style=\"text-align: right;\">       4.90342</td><td style=\"text-align: right;\"> 1675093804</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>9b6c5_00006</td><td style=\"text-align: right;\">   0.00308251</td></tr>\n",
       "<tr><td>train_model_9b6c5_00007</td><td style=\"text-align: right;\">81424.7  </td><td>2023-01-30_23-50-11</td><td>True  </td><td>                </td><td>9954c156fb2449cea49c83d5c3f037ec</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\">32932</td><td style=\"text-align: right;\">             1.11217</td><td style=\"text-align: right;\">          0.330431</td><td style=\"text-align: right;\">       1.11217</td><td style=\"text-align: right;\"> 1675093811</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>9b6c5_00007</td><td style=\"text-align: right;\">   0.00402522</td></tr>\n",
       "<tr><td>train_model_9b6c5_00008</td><td style=\"text-align: right;\">81424.7  </td><td>2023-01-30_23-50-18</td><td>True  </td><td>                </td><td>0916c18cf9104df09128bc344025199b</td><td>                                            </td><td>Tensortorch</td><td style=\"text-align: right;\">                         3</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 4148</td><td style=\"text-align: right;\">             1.1908 </td><td style=\"text-align: right;\">          0.387702</td><td style=\"text-align: right;\">       1.1908 </td><td style=\"text-align: right;\"> 1675093818</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>9b6c5_00008</td><td style=\"text-align: right;\">   0.0039804 </td></tr>\n",
       "<tr><td>train_model_9b6c5_00009</td><td style=\"text-align: right;\">  538.154</td><td>2023-01-30_23-50-40</td><td>True  </td><td>                </td><td>d7c429ae52db469a9fcbc4d70a7d081b</td><td>9_batch_size=16,dropout=0.2991,hidden_dim=30</td><td>Tensortorch</td><td style=\"text-align: right;\">                        28</td><td>127.0.0.1</td><td style=\"text-align: right;\">28748</td><td style=\"text-align: right;\">            16.3461 </td><td style=\"text-align: right;\">          0.518004</td><td style=\"text-align: right;\">      16.3461 </td><td style=\"text-align: right;\"> 1675093840</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  28</td><td>9b6c5_00009</td><td style=\"text-align: right;\">   0.00407863</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:04 (running for 00:00:06.46)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=28994.706076765517 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status   | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_9b6c5_00000 | RUNNING  | 127.0.0.1:11872 |           32 |           60 | 0.283982  | 28994.7 |                    1 |\n",
      "| train_model_9b6c5_00001 | PENDING  |                 |           16 |           60 | 0.0597332 |         |                      |\n",
      "| train_model_9b6c5_00002 | PENDING  |                 |          128 |           90 | 0.218672  |         |                      |\n",
      "| train_model_9b6c5_00003 | PENDING  |                 |           64 |           60 | 0.199901  |         |                      |\n",
      "| train_model_9b6c5_00004 | PENDING  |                 |          128 |           60 | 0.0941246 |         |                      |\n",
      "| train_model_9b6c5_00005 | PENDING  |                 |           16 |           60 | 0.0678516 |         |                      |\n",
      "| train_model_9b6c5_00006 | PENDING  |                 |           16 |           90 | 0.295774  |         |                      |\n",
      "| train_model_9b6c5_00007 | PENDING  |                 |          128 |           30 | 0.141403  |         |                      |\n",
      "| train_model_9b6c5_00008 | PENDING  |                 |          128 |           30 | 0.147392  |         |                      |\n",
      "| train_model_9b6c5_00009 | PENDING  |                 |           16 |           30 | 0.299093  |         |                      |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:03,998\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:04,468\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:04,930\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:05,407\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:05,860\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:06,332\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:06,788\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:07,242\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:07,704\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:08,162\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:08,625\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:09 (running for 00:00:11.53)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -1515.8067055893919 | Iter 3.000: -3849.3930138904557\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1239.0354058248513 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status   | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_9b6c5_00000 | RUNNING  | 127.0.0.1:11872 |           32 |           60 | 0.283982  | 1239.04 |                   12 |\n",
      "| train_model_9b6c5_00001 | PENDING  |                 |           16 |           60 | 0.0597332 |         |                      |\n",
      "| train_model_9b6c5_00002 | PENDING  |                 |          128 |           90 | 0.218672  |         |                      |\n",
      "| train_model_9b6c5_00003 | PENDING  |                 |           64 |           60 | 0.199901  |         |                      |\n",
      "| train_model_9b6c5_00004 | PENDING  |                 |          128 |           60 | 0.0941246 |         |                      |\n",
      "| train_model_9b6c5_00005 | PENDING  |                 |           16 |           60 | 0.0678516 |         |                      |\n",
      "| train_model_9b6c5_00006 | PENDING  |                 |           16 |           90 | 0.295774  |         |                      |\n",
      "| train_model_9b6c5_00007 | PENDING  |                 |          128 |           30 | 0.141403  |         |                      |\n",
      "| train_model_9b6c5_00008 | PENDING  |                 |          128 |           30 | 0.147392  |         |                      |\n",
      "| train_model_9b6c5_00009 | PENDING  |                 |           16 |           30 | 0.299093  |         |                      |\n",
      "+-------------------------+----------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:09,074\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:09,539\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:10,001\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:10,462\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:10,928\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:11,389\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:11,854\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:12,309\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:12,763\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=11872)\u001b[0m 2023-01-30 23:49:13,221\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:18 (running for 00:00:21.27)\n",
      "Memory usage on this node: 18.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -1515.8067055893919 | Iter 3.000: -3849.3930138904557\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_9b6c5_00001 | RUNNING    | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |         |                      |\n",
      "| train_model_9b6c5_00002 | PENDING    |                 |          128 |           90 | 0.218672  |         |                      |\n",
      "| train_model_9b6c5_00003 | PENDING    |                 |           64 |           60 | 0.199901  |         |                      |\n",
      "| train_model_9b6c5_00004 | PENDING    |                 |          128 |           60 | 0.0941246 |         |                      |\n",
      "| train_model_9b6c5_00005 | PENDING    |                 |           16 |           60 | 0.0678516 |         |                      |\n",
      "| train_model_9b6c5_00006 | PENDING    |                 |           16 |           90 | 0.295774  |         |                      |\n",
      "| train_model_9b6c5_00007 | PENDING    |                 |          128 |           30 | 0.141403  |         |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |         |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |         |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  | 1246.77 |                   21 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05973317470990048 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 3 | rnn           | LSTM             | 17.0 K\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 4 | V             | Linear           | 122   \n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 17.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 17.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 0.137     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 2023-01-30 23:49:19,679\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 2023-01-30 23:49:20,294\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 2023-01-30 23:49:20,868\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 2023-01-30 23:49:21,457\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 2023-01-30 23:49:22,038\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32088)\u001b[0m 2023-01-30 23:49:22,617\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:27 (running for 00:00:30.31)\n",
      "Memory usage on this node: 18.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -1968.9955890796482 | Iter 3.000: -3161.9255729952233\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |    MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------|\n",
      "| train_model_9b6c5_00002 | RUNNING    | 127.0.0.1:34416 |          128 |           90 | 0.218672  |         |                      |\n",
      "| train_model_9b6c5_00003 | PENDING    |                 |           64 |           60 | 0.199901  |         |                      |\n",
      "| train_model_9b6c5_00004 | PENDING    |                 |          128 |           60 | 0.0941246 |         |                      |\n",
      "| train_model_9b6c5_00005 | PENDING    |                 |           16 |           60 | 0.0678516 |         |                      |\n",
      "| train_model_9b6c5_00006 | PENDING    |                 |           16 |           90 | 0.295774  |         |                      |\n",
      "| train_model_9b6c5_00007 | PENDING    |                 |          128 |           30 | 0.141403  |         |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |         |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |         |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  | 1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 | 2422.18 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+---------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.21867238406118275 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 3 | rnn           | LSTM             | 36.4 K\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 4 | V             | Linear           | 182   \n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 36.5 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 36.5 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 0.292     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 2023-01-30 23:49:28,703\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 2023-01-30 23:49:29,258\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=34416)\u001b[0m 2023-01-30 23:49:29,802\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:34 (running for 00:00:37.32)\n",
      "Memory usage on this node: 18.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -1968.9955890796482 | Iter 3.000: -3849.3930138904557\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00003 | RUNNING    | 127.0.0.1:476   |           64 |           60 | 0.199901  |          |                      |\n",
      "| train_model_9b6c5_00004 | PENDING    |                 |          128 |           60 | 0.0941246 |          |                      |\n",
      "| train_model_9b6c5_00005 | PENDING    |                 |           16 |           60 | 0.0678516 |          |                      |\n",
      "| train_model_9b6c5_00006 | PENDING    |                 |           16 |           90 | 0.295774  |          |                      |\n",
      "| train_model_9b6c5_00007 | PENDING    |                 |          128 |           30 | 0.141403  |          |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |          |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1999005412057655 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 3 | rnn           | LSTM             | 17.0 K\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 4 | V             | Linear           | 122   \n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 17.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 17.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 0.137     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 2023-01-30 23:49:35,630\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 2023-01-30 23:49:36,028\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=476)\u001b[0m 2023-01-30 23:49:36,439\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:41 (running for 00:00:44.39)\n",
      "Memory usage on this node: 18.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -1968.9955890796482 | Iter 3.000: -10131.559709283765\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00004 | RUNNING    | 127.0.0.1:18048 |          128 |           60 | 0.0941246 |          |                      |\n",
      "| train_model_9b6c5_00005 | PENDING    |                 |           16 |           60 | 0.0678516 |          |                      |\n",
      "| train_model_9b6c5_00006 | PENDING    |                 |           16 |           90 | 0.295774  |          |                      |\n",
      "| train_model_9b6c5_00007 | PENDING    |                 |          128 |           30 | 0.141403  |          |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |          |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7  |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.09412458740994041 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 3 | rnn           | LSTM             | 17.0 K\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 4 | V             | Linear           | 122   \n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 17.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 17.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 0.137     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 2023-01-30 23:49:42,688\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 2023-01-30 23:49:43,071\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=18048)\u001b[0m 2023-01-30 23:49:43,458\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:48 (running for 00:00:51.41)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -1968.9955890796482 | Iter 3.000: -16413.726404677076\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00005 | RUNNING    | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |          |                      |\n",
      "| train_model_9b6c5_00006 | PENDING    |                 |           16 |           90 | 0.295774  |          |                      |\n",
      "| train_model_9b6c5_00007 | PENDING    |                 |          128 |           30 | 0.141403  |          |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |          |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7  |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1  |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.06785158426795594 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 3 | rnn           | LSTM             | 17.0 K\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 4 | V             | Linear           | 122   \n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 17.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 17.2 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 0.137     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 2023-01-30 23:49:50,098\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 2023-01-30 23:49:50,707\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 2023-01-30 23:49:51,330\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 2023-01-30 23:49:51,928\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 2023-01-30 23:49:52,513\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=27488)\u001b[0m 2023-01-30 23:49:53,112\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:49:54 (running for 00:00:56.45)\n",
      "Memory usage on this node: 18.1/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -10131.559709283765\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00006 | RUNNING    | 127.0.0.1:1548  |           16 |           90 | 0.295774  |          |                      |\n",
      "| train_model_9b6c5_00007 | PENDING    |                 |          128 |           30 | 0.141403  |          |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |          |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7  |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1  |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2957743838545088 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 3 | rnn           | LSTM             | 36.4 K\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 4 | V             | Linear           | 182   \n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 36.5 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 36.5 K    Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 0.292     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:00 (running for 00:01:02.57)\n",
      "Memory usage on this node: 18.5/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -10131.559709283765\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00006 | RUNNING    | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  6042.93 |                    1 |\n",
      "| train_model_9b6c5_00007 | PENDING    |                 |          128 |           30 | 0.141403  |          |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |          |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7  |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1  |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 2023-01-30 23:50:00,112\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 2023-01-30 23:50:00,902\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 2023-01-30 23:50:01,684\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 2023-01-30 23:50:02,432\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 2023-01-30 23:50:03,203\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=1548)\u001b[0m 2023-01-30 23:50:04,078\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:10 (running for 00:01:12.52)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -4194.62000862932\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00007 | RUNNING    | 127.0.0.1:32932 |          128 |           30 | 0.141403  |          |                      |\n",
      "| train_model_9b6c5_00008 | PENDING    |                 |          128 |           30 | 0.147392  |          |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7  |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1  |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00006 | TERMINATED | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  3841.69 |                    6 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1414031791881643 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 3 | rnn           | LSTM             | 4.9 K \n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 4 | V             | Linear           | 62    \n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 5.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 5.0 K     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 0.040     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 2023-01-30 23:50:10,652\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 2023-01-30 23:50:10,980\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=32932)\u001b[0m 2023-01-30 23:50:11,310\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:17 (running for 00:01:19.60)\n",
      "Memory usage on this node: 18.3/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -10304.173206653199\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00008 | RUNNING    | 127.0.0.1:4148  |          128 |           30 | 0.147392  |          |                      |\n",
      "| train_model_9b6c5_00009 | PENDING    |                 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7  |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1  |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00006 | TERMINATED | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  3841.69 |                    6 |\n",
      "| train_model_9b6c5_00007 | TERMINATED | 127.0.0.1:32932 |          128 |           30 | 0.141403  | 81424.7  |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.14739232210579573 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 3 | rnn           | LSTM             | 4.9 K \n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 4 | V             | Linear           | 62    \n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 5.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 5.0 K     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 0.040     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 2023-01-30 23:50:17,577\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 2023-01-30 23:50:17,920\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=4148)\u001b[0m 2023-01-30 23:50:18,307\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:24 (running for 00:01:26.64)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -16413.726404677076\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00000 with MAPE=1246.7674651815958 and parameters={'batch_size': 32, 'hidden_dim': 60, 'dropout': 0.28398153787733454}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |     MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------|\n",
      "| train_model_9b6c5_00009 | RUNNING    | 127.0.0.1:28748 |           16 |           30 | 0.299093  |          |                      |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77 |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3  |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7  |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1  |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18 |                    6 |\n",
      "| train_model_9b6c5_00006 | TERMINATED | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  3841.69 |                    6 |\n",
      "| train_model_9b6c5_00007 | TERMINATED | 127.0.0.1:32932 |          128 |           30 | 0.141403  | 81424.7  |                    3 |\n",
      "| train_model_9b6c5_00008 | TERMINATED | 127.0.0.1:4148  |          128 |           30 | 0.147392  | 81424.7  |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.29909326760922184 and num_layers=1\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m GPU available: True (cuda), used: False\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m c:\\Users\\daeky\\miniconda3\\envs\\darts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m   | Name          | Type             | Params\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 0 | criterion     | MSELoss          | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 1 | train_metrics | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2 | val_metrics   | MetricCollection | 0     \n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 3 | rnn           | LSTM             | 4.9 K \n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 4 | V             | Linear           | 62    \n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m ---------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 5.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 5.0 K     Total params\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 0.040     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:25,125\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:25,671\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:26,237\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:26,780\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:27,328\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:27,945\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:28,488\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:29,138\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:29 (running for 00:01:32.36)\n",
      "Memory usage on this node: 18.5/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1239.0354058248513 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -10304.173206653199\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00009 with MAPE=850.7757235917486 and parameters={'batch_size': 16, 'hidden_dim': 30, 'dropout': 0.29909326760922184}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |      MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------|\n",
      "| train_model_9b6c5_00009 | RUNNING    | 127.0.0.1:28748 |           16 |           30 | 0.299093  |   850.776 |                    9 |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77  |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3   |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7   |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1   |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00006 | TERMINATED | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  3841.69  |                    6 |\n",
      "| train_model_9b6c5_00007 | TERMINATED | 127.0.0.1:32932 |          128 |           30 | 0.141403  | 81424.7   |                    3 |\n",
      "| train_model_9b6c5_00008 | TERMINATED | 127.0.0.1:4148  |          128 |           30 | 0.147392  | 81424.7   |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:29,908\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:30,594\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:31,132\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:31,815\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:32,388\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:32,923\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:33,539\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:34,049\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:34,611\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:35 (running for 00:01:37.60)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: -1131.7847706099685 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -10304.173206653199\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00009 with MAPE=661.1484879106773 and parameters={'batch_size': 16, 'hidden_dim': 30, 'dropout': 0.29909326760922184}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |      MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------|\n",
      "| train_model_9b6c5_00009 | RUNNING    | 127.0.0.1:28748 |           16 |           30 | 0.299093  |   661.148 |                   18 |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77  |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3   |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7   |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1   |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00006 | TERMINATED | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  3841.69  |                    6 |\n",
      "| train_model_9b6c5_00007 | TERMINATED | 127.0.0.1:32932 |          128 |           30 | 0.141403  | 81424.7   |                    3 |\n",
      "| train_model_9b6c5_00008 | TERMINATED | 127.0.0.1:4148  |          128 |           30 | 0.147392  | 81424.7   |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:35,143\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:35,876\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:36,456\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:36,952\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:37,469\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:38,060\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:38,575\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:39,106\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:39,655\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:40 (running for 00:01:42.65)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: -763.5910318452559 | Iter 12.000: -1131.7847706099685 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -10304.173206653199\n",
      "Resources requested: 16.0/20 CPUs, 1.0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00009 with MAPE=552.8240614366725 and parameters={'batch_size': 16, 'hidden_dim': 30, 'dropout': 0.29909326760922184}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |      MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------|\n",
      "| train_model_9b6c5_00009 | RUNNING    | 127.0.0.1:28748 |           16 |           30 | 0.299093  |   552.824 |                   27 |\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77  |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3   |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7   |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1   |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00006 | TERMINATED | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  3841.69  |                    6 |\n",
      "| train_model_9b6c5_00007 | TERMINATED | 127.0.0.1:32932 |          128 |           30 | 0.141403  | 81424.7   |                    3 |\n",
      "| train_model_9b6c5_00008 | TERMINATED | 127.0.0.1:4148  |          128 |           30 | 0.147392  | 81424.7   |                    3 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:40,197\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[2m\u001b[36m(train_model pid=28748)\u001b[0m 2023-01-30 23:50:40,715\tWARNING pytorch_lightning.py:134 -- Metric val_Loss does not exist in `trainer.callback_metrics.\n",
      "2023-01-30 23:50:40,992\tINFO tune.py:762 -- Total run time: 103.45 seconds (103.30 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-30 23:50:40 (running for 00:01:43.32)\n",
      "Memory usage on this node: 18.4/31.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 768.000: None | Iter 384.000: None | Iter 192.000: None | Iter 96.000: None | Iter 48.000: None | Iter 24.000: -763.5910318452559 | Iter 12.000: -1131.7847706099685 | Iter 6.000: -2422.1844725699048 | Iter 3.000: -10304.173206653199\n",
      "Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/9.84 GiB heap, 0.0/4.92 GiB objects\n",
      "Current best trial: 9b6c5_00009 with MAPE=538.154337665045 and parameters={'batch_size': 16, 'hidden_dim': 30, 'dropout': 0.29909326760922184}\n",
      "Result logdir: C:\\Users\\daeky\\ray_results\\tune_darts\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "| Trial name              | status     | loc             |   batch_size |   hidden_dim |   dropout |      MAPE |   training_iteration |\n",
      "|-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------|\n",
      "| train_model_9b6c5_00000 | TERMINATED | 127.0.0.1:11872 |           32 |           60 | 0.283982  |  1246.77  |                   21 |\n",
      "| train_model_9b6c5_00001 | TERMINATED | 127.0.0.1:32088 |           16 |           60 | 0.0597332 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00002 | TERMINATED | 127.0.0.1:34416 |          128 |           90 | 0.218672  | 21652.3   |                    3 |\n",
      "| train_model_9b6c5_00003 | TERMINATED | 127.0.0.1:476   |           64 |           60 | 0.199901  | 16413.7   |                    3 |\n",
      "| train_model_9b6c5_00004 | TERMINATED | 127.0.0.1:18048 |          128 |           60 | 0.0941246 | 39777.1   |                    3 |\n",
      "| train_model_9b6c5_00005 | TERMINATED | 127.0.0.1:27488 |           16 |           60 | 0.0678516 |  2422.18  |                    6 |\n",
      "| train_model_9b6c5_00006 | TERMINATED | 127.0.0.1:1548  |           16 |           90 | 0.295774  |  3841.69  |                    6 |\n",
      "| train_model_9b6c5_00007 | TERMINATED | 127.0.0.1:32932 |          128 |           30 | 0.141403  | 81424.7   |                    3 |\n",
      "| train_model_9b6c5_00008 | TERMINATED | 127.0.0.1:4148  |          128 |           30 | 0.147392  | 81424.7   |                    3 |\n",
      "| train_model_9b6c5_00009 | TERMINATED | 127.0.0.1:28748 |           16 |           30 | 0.299093  |   538.154 |                   28 |\n",
      "+-------------------------+------------+-----------------+--------------+--------------+-----------+-----------+----------------------+\n",
      "\n",
      "\n",
      "Best hyperparameters found were:  {'batch_size': 16, 'hidden_dim': 30, 'dropout': 0.29909326760922184}\n"
     ]
    }
   ],
   "source": [
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_MeanAbsolutePercentageError\",\n",
    "    patience=5,\n",
    "    min_delta=0.05,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# set up ray tune callback\n",
    "tune_callback = TuneReportCallback(\n",
    "    {\n",
    "        \"loss\": \"val_Loss\",\n",
    "        \"MAPE\": \"val_MeanAbsolutePercentageError\",\n",
    "    },\n",
    "    on=\"validation_end\",\n",
    ")\n",
    "\n",
    "# define the hyperparameter space\n",
    "config = {\n",
    "    \"batch_size\": tune.choice([16, 32, 64, 128]),\n",
    "    \"hidden_dim\": tune.choice([30, 60, 90]),\n",
    "    \"dropout\": tune.uniform(0, 0.3),\n",
    "}\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=list(config.keys()),\n",
    "    metric_columns=[\"loss\", \"MAPE\", \"training_iteration\"],\n",
    ")\n",
    "\n",
    "resources_per_trial = {\"cpu\": 16, \"gpu\": 1}\n",
    "\n",
    "# the number of combinations to try\n",
    "num_samples = 10\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=1000, grace_period=3, reduction_factor=2)\n",
    "\n",
    "train_fn_with_parameters = tune.with_parameters(\n",
    "    train_model, callbacks=[my_stopper, tune_callback], train=train, val=val, covariates=covariates\n",
    ")\n",
    "\n",
    "# optimize hyperparameters by minimizing the MAPE on the validation set\n",
    "analysis = tune.run(\n",
    "    train_fn_with_parameters,\n",
    "    resources_per_trial=resources_per_trial,\n",
    "    # Using a metric instead of loss allows for\n",
    "    # comparison between different likelihood or loss functions.\n",
    "    metric=\"MAPE\",  # any value in TuneReportCallback.\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    name=\"tune_darts\",\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  1%|          | 1/100 [00:25<41:22, 25.07s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  2%|         | 2/100 [00:51<42:13, 25.85s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  3%|         | 3/100 [01:18<42:21, 26.20s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  4%|         | 4/100 [01:43<41:21, 25.84s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  5%|         | 5/100 [02:11<42:01, 26.54s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  6%|         | 6/100 [02:38<41:49, 26.69s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  7%|         | 7/100 [03:04<41:06, 26.52s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  8%|         | 8/100 [03:29<40:06, 26.16s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  9%|         | 9/100 [03:55<39:29, 26.04s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 10%|         | 10/100 [04:21<38:52, 25.91s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 11%|         | 11/100 [04:46<38:06, 25.70s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 12%|        | 12/100 [05:12<37:55, 25.85s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 13%|        | 13/100 [05:38<37:22, 25.77s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 14%|        | 14/100 [06:03<36:44, 25.64s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 15%|        | 15/100 [06:28<35:57, 25.38s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 16%|        | 16/100 [06:53<35:30, 25.36s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 17%|        | 17/100 [07:18<35:05, 25.37s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 18%|        | 18/100 [07:44<34:51, 25.50s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 19%|        | 19/100 [08:10<34:34, 25.61s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 20%|        | 20/100 [08:36<34:08, 25.60s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 21%|        | 21/100 [09:01<33:39, 25.56s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 22%|       | 22/100 [09:26<33:08, 25.49s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 23%|       | 23/100 [09:52<32:33, 25.37s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 24%|       | 24/100 [10:17<32:07, 25.36s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 25%|       | 25/100 [10:41<31:19, 25.05s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 26%|       | 26/100 [11:05<30:30, 24.74s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 27%|       | 27/100 [11:30<30:13, 24.84s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 28%|       | 28/100 [11:55<29:44, 24.79s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 29%|       | 29/100 [12:19<29:12, 24.68s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 30%|       | 30/100 [12:44<28:37, 24.54s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 31%|       | 31/100 [13:08<28:07, 24.46s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 32%|      | 32/100 [13:35<28:37, 25.25s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 33%|      | 33/100 [14:00<27:59, 25.07s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 34%|      | 34/100 [14:26<28:03, 25.50s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 35%|      | 35/100 [14:53<28:00, 25.85s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 36%|      | 36/100 [15:20<27:57, 26.21s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 37%|      | 37/100 [15:46<27:34, 26.27s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 38%|      | 38/100 [16:12<26:55, 26.05s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 39%|      | 39/100 [16:40<27:06, 26.66s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 40%|      | 40/100 [17:07<26:52, 26.88s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 41%|      | 41/100 [17:34<26:29, 26.94s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 42%|     | 42/100 [18:01<25:54, 26.81s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 43%|     | 43/100 [18:25<24:47, 26.10s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 44%|     | 44/100 [18:53<24:44, 26.51s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 45%|     | 45/100 [19:23<25:18, 27.61s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 46%|     | 46/100 [19:52<25:16, 28.08s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 47%|     | 47/100 [20:20<24:40, 27.94s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 48%|     | 48/100 [20:45<23:30, 27.12s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 49%|     | 49/100 [21:12<22:57, 27.01s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 50%|     | 50/100 [21:41<23:07, 27.75s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 51%|     | 51/100 [22:12<23:24, 28.67s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 52%|    | 52/100 [22:40<22:44, 28.43s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 53%|    | 53/100 [23:08<22:10, 28.30s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 54%|    | 54/100 [23:36<21:43, 28.34s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 55%|    | 55/100 [24:05<21:17, 28.38s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 56%|    | 56/100 [24:36<21:27, 29.26s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 57%|    | 57/100 [25:02<20:13, 28.22s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 58%|    | 58/100 [25:30<19:43, 28.18s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 59%|    | 59/100 [25:57<19:00, 27.81s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 60%|    | 60/100 [26:23<18:10, 27.27s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 61%|    | 61/100 [26:50<17:35, 27.07s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 62%|   | 62/100 [27:17<17:13, 27.21s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 63%|   | 63/100 [27:45<16:53, 27.40s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 64%|   | 64/100 [28:11<16:15, 27.09s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 65%|   | 65/100 [28:39<15:59, 27.40s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 66%|   | 66/100 [29:06<15:19, 27.04s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 67%|   | 67/100 [29:35<15:16, 27.76s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 68%|   | 68/100 [30:07<15:26, 28.94s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 69%|   | 69/100 [30:34<14:38, 28.34s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 70%|   | 70/100 [31:01<13:57, 27.93s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 71%|   | 71/100 [31:28<13:29, 27.91s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 72%|  | 72/100 [31:56<13:00, 27.87s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 73%|  | 73/100 [32:24<12:33, 27.89s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 74%|  | 74/100 [32:52<12:07, 27.98s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 75%|  | 75/100 [33:21<11:41, 28.05s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 76%|  | 76/100 [33:50<11:25, 28.56s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 77%|  | 77/100 [34:20<11:04, 28.88s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 78%|  | 78/100 [34:47<10:22, 28.30s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 79%|  | 79/100 [35:15<09:52, 28.24s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 80%|  | 80/100 [35:45<09:37, 28.90s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 81%|  | 81/100 [36:14<09:05, 28.71s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 82%| | 82/100 [36:42<08:35, 28.67s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 83%| | 83/100 [37:12<08:13, 29.06s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 84%| | 84/100 [37:40<07:39, 28.73s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 85%| | 85/100 [38:07<07:03, 28.26s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 86%| | 86/100 [38:36<06:38, 28.46s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 87%| | 87/100 [39:06<06:13, 28.72s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 88%| | 88/100 [39:36<05:48, 29.06s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 89%| | 89/100 [40:04<05:18, 29.00s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 90%| | 90/100 [40:32<04:45, 28.59s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 91%| | 91/100 [41:01<04:18, 28.77s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 92%|| 92/100 [41:27<03:42, 27.84s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 93%|| 93/100 [41:52<03:09, 27.03s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 94%|| 94/100 [42:18<02:39, 26.60s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 95%|| 95/100 [42:42<02:09, 25.99s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 96%|| 96/100 [43:09<01:44, 26.12s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 97%|| 97/100 [43:34<01:17, 25.79s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 98%|| 98/100 [43:59<00:51, 25.71s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      " 99%|| 99/100 [44:24<00:25, 25.34s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "100%|| 100/100 [44:49<00:00, 26.89s/it]\n"
     ]
    }
   ],
   "source": [
    "smape_list = []\n",
    "rmspe_list = []\n",
    "\n",
    "for num in tqdm(store_num_list):\n",
    "    train_transformed, test_transformed, covariates = get_sample(num)\n",
    "    rnn_model = RNNModel(\n",
    "            model=\"LSTM\",\n",
    "            hidden_dim=30,\n",
    "            dropout= 0.29909326760922184,\n",
    "            batch_size=128,\n",
    "            n_epochs=100,\n",
    "            optimizer_kwargs={\"lr\": 1e-3},\n",
    "            model_name=\"rnn_sales\",\n",
    "            random_state=42,\n",
    "            training_length=30,\n",
    "            input_chunk_length=28,\n",
    "            force_reset=True,\n",
    "            save_checkpoints=True,\n",
    "            pl_trainer_kwargs ={\"accelerator\": \"gpu\", \"devices\": [0],  \n",
    "            \"enable_progress_bar\": False}\n",
    "    )\n",
    "    mape_score, rmse_score = evaluate_covariates_model(rnn_model, train_transformed, test_transformed, covariates )\n",
    "    smape_list.append(mape_score)\n",
    "    rmspe_list.append(rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmspe</th>\n",
       "      <th>smape</th>\n",
       "      <th>Store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.096617</td>\n",
       "      <td>8.537655</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062003</td>\n",
       "      <td>5.227861</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.058157</td>\n",
       "      <td>4.356692</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110809</td>\n",
       "      <td>7.747403</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063912</td>\n",
       "      <td>4.945696</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.056104</td>\n",
       "      <td>4.791607</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.068267</td>\n",
       "      <td>5.762917</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.038038</td>\n",
       "      <td>3.219110</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.083961</td>\n",
       "      <td>7.677319</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.082228</td>\n",
       "      <td>6.412379</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rmspe     smape  Store\n",
       "0   0.096617  8.537655     10\n",
       "1   0.062003  5.227861     62\n",
       "2   0.058157  4.356692    110\n",
       "3   0.110809  7.747403    163\n",
       "4   0.063912  4.945696    300\n",
       "..       ...       ...    ...\n",
       "95  0.056104  4.791607    960\n",
       "96  0.068267  5.762917   1010\n",
       "97  0.038038  3.219110   1046\n",
       "98  0.083961  7.677319   1089\n",
       "99  0.082228  6.412379   1101\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_h2 = pd.DataFrame(\n",
    "    {\n",
    "        'rmspe' : rmspe_list,\n",
    "        'smape' : smape_list,\n",
    "    }\n",
    ")\n",
    "res_h2['Store'] = store_num_list\n",
    "res_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store      585.390000\n",
       "rmspe_x      0.076045\n",
       "smape_x      6.459042\n",
       "rmspe_y      0.086656\n",
       "smape_y      7.037758\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.merge(res_h1, res_h2, left_on='Store', right_on='Store')\n",
    "result = result.set_index('Store')\n",
    "result = pd.merge(result, store[['Store','StoreType']], left_on='Store', right_on='Store')\n",
    "result.to_csv('../output/rnn_hyper_result.csv')\n",
    "result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>rmspe_x</th>\n",
       "      <th>smape_x</th>\n",
       "      <th>rmspe_y</th>\n",
       "      <th>smape_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StoreType</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>617.333333</td>\n",
       "      <td>0.072249</td>\n",
       "      <td>6.039484</td>\n",
       "      <td>0.082313</td>\n",
       "      <td>6.875778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>528.600000</td>\n",
       "      <td>0.140178</td>\n",
       "      <td>11.943553</td>\n",
       "      <td>0.196384</td>\n",
       "      <td>14.794481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>605.133333</td>\n",
       "      <td>0.068919</td>\n",
       "      <td>5.822658</td>\n",
       "      <td>0.074484</td>\n",
       "      <td>6.128657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>552.633333</td>\n",
       "      <td>0.065591</td>\n",
       "      <td>5.686814</td>\n",
       "      <td>0.066595</td>\n",
       "      <td>5.523264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Store   rmspe_x    smape_x   rmspe_y    smape_y\n",
       "StoreType                                                      \n",
       "a          617.333333  0.072249   6.039484  0.082313   6.875778\n",
       "b          528.600000  0.140178  11.943553  0.196384  14.794481\n",
       "c          605.133333  0.068919   5.822658  0.074484   6.128657\n",
       "d          552.633333  0.065591   5.686814  0.066595   5.523264"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby('StoreType').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (default, Mar  2 2020, 13:06:26) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e4eb80f0a2947b2b4d080aa13c6d15fa0bc013de0be3eb24296c75a14756372"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
